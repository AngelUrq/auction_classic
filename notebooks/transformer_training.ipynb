{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3364,
     "status": "ok",
     "timestamp": 1729270089178,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "BVwlGTGCcJRb",
    "outputId": "7b71789e-e97c-463f-ec99-87523000df9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_dir = \"../\"\n",
    "zip_file_path = \"generated.zip\"\n",
    "\n",
    "if not os.path.exists(target_dir + 'generated'):\n",
    "    print(f\"The directory {target_dir} does not exist. Proceeding with download.\")\n",
    "\n",
    "    !apt-get update\n",
    "    !apt-get install unzip\n",
    "    \n",
    "    !curl \"https://drive.usercontent.google.com/download?id=1SJkXUcdWqPvhBO0Ug5SlJYRvdSn9eY22&confirm=xxx\" -o {zip_file_path}\n",
    "    !mkdir -p {target_dir}\n",
    "    \n",
    "    !unzip {zip_file_path} -d {target_dir}\n",
    "    \n",
    "    print(f\"File downloaded and extracted to {target_dir}\")\n",
    "    \n",
    "    !rm {zip_file_path}\n",
    "else:\n",
    "    print(f\"The directory {target_dir} already exists. No action taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8394,
     "status": "ok",
     "timestamp": 1729270097569,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "gsqGICHeE47W",
    "outputId": "5fbdf5a0-d09a-40e1-a47a-2a80b8007caa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().parent.resolve()\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data.auction_dataset import AuctionDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 16985,
     "status": "ok",
     "timestamp": 1729270137493,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "KjcQtWtoE47Y",
    "outputId": "12bf3488-0c75-42ff-b77c-f5dc88a524e0"
   },
   "outputs": [],
   "source": [
    "pairs = pd.read_csv('../generated/auction_indices.csv')\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_XAyq9XvHHu"
   },
   "source": [
    "## Prepare and balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 2282,
     "status": "ok",
     "timestamp": 1729270139773,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "BeSUcQqnvrUH",
    "outputId": "324777e2-c906-482f-9349-d56e1945373f"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pairs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 2725,
     "status": "ok",
     "timestamp": 1729270142496,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "knMeDyMD6Sys",
    "outputId": "25d8fe21-716d-4bfe-a81b-977e6046fb57"
   },
   "outputs": [],
   "source": [
    "pairs = pairs[pairs['g_hours_on_sale_max'] < 50]\n",
    "pairs = pairs[pairs['g_current_hours_max'] < 50]\n",
    "\n",
    "pairs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1729270143003,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "CVaV2INNE47Y",
    "outputId": "53d471b4-79a7-4a01-8f8b-423de5afdb84"
   },
   "outputs": [],
   "source": [
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.15, random_state=42, shuffle=False)\n",
    "\n",
    "print(f\"Before filtering: {len(train_pairs)}\")\n",
    "\n",
    "train_pairs = train_pairs[train_pairs['g_hours_on_sale_len'] <= 32]\n",
    "val_pairs = val_pairs[val_pairs['g_hours_on_sale_len'] <= 32]\n",
    "\n",
    "print(f\"After filtering: {len(train_pairs)}\\n\")\n",
    "\n",
    "train_pairs = train_pairs[:int(len(train_pairs)*0.9)]\n",
    "val_pairs = val_pairs[len(val_pairs)//2:]\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "print(f\"Val pairs: {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of group_hours_on_sale_mean\n",
    "plt.hist(train_pairs['g_hours_on_sale_mean'], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_pairs['g_hours_on_sale_len'], bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729270149598,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "oRrffN74vIhi",
    "outputId": "cd8b1691-8251-4840-f6ea-3c14f979c1d5"
   },
   "outputs": [],
   "source": [
    "plt.hist(train_pairs['g_hours_on_sale_mean'], bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "mappings_dir = '../generated/mappings'\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'item_to_idx.json'), 'r') as f:\n",
    "    item_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'context_to_idx.json'), 'r') as f:\n",
    "    context_to_idx = json.load(f)\n",
    "    \n",
    "with open(os.path.join(mappings_dir, 'bonus_to_idx.json'), 'r') as f:\n",
    "    bonus_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'modtype_to_idx.json'), 'r') as f:\n",
    "    modtype_to_idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats = torch.load('../generated/feature_stats.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729270151907,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "-ndNrhFcUmFL",
    "outputId": "6962e361-1b92-4da0-b0f0-58a39cafe50f"
   },
   "outputs": [],
   "source": [
    "from src.data.auction_dataset import AuctionDataset\n",
    "from src.data.utils import collate_auctions\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = AuctionDataset(train_pairs, feature_stats=feature_stats, path='../generated/sequences.h5')\n",
    "val_dataset = AuctionDataset(val_pairs, feature_stats=feature_stats, path='../generated/sequences.h5')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_auctions, num_workers=8, prefetch_factor=8, pin_memory=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_auctions, num_workers=8, prefetch_factor=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcSVYngOnnZA"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = True\n",
    "\n",
    "if test_data_loader:\n",
    "    iter_loader = iter(train_dataloader)\n",
    "    (auctions, item_index, contexts, bonus_lists, modifier_types, modifier_values, current_hours), y = next(iter_loader)\n",
    "\n",
    "    print(f'auctions: {auctions.shape}')\n",
    "    print(f'item_index: {item_index.shape}')\n",
    "    print(f'contexts: {contexts.shape}')\n",
    "    print(f'bonus_lists: {bonus_lists.shape}')\n",
    "    print(f'modifier_types: {modifier_types.shape}')\n",
    "    print(f'modifier_values: {modifier_values.shape}')\n",
    "    print(f'current_hours: {current_hours.shape}')\n",
    "    print(f'y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.auction_transformer import AuctionTransformer\n",
    "\n",
    "input_size = 7\n",
    "embedding_dim = 64  # Increase from 32\n",
    "d_model = 512       # Increase from 256\n",
    "dim_feedforward = d_model * 4  # This will automatically scale with d_model\n",
    "nhead = 16          # Increase from 8 (should be a divisor of d_model)\n",
    "num_layers = 12     # Increase from 8\n",
    "dropout_p = 0.1\n",
    "n_items = len(item_to_idx)\n",
    "n_contexts = len(context_to_idx) + 1\n",
    "n_bonuses = len(bonus_to_idx)\n",
    "n_modtypes = len(modtype_to_idx)\n",
    "\n",
    "model = AuctionTransformer(\n",
    "    input_size, \n",
    "    n_items, \n",
    "    n_contexts,\n",
    "    n_bonuses,\n",
    "    n_modtypes,\n",
    "    embedding_dim, \n",
    "    d_model, \n",
    "    dim_feedforward, \n",
    "    nhead, \n",
    "    num_layers,\n",
    "    dropout_p=dropout_p,\n",
    "    learning_rate=3e-5,\n",
    "    logging_interval=500\n",
    ")\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyMw3icYnqXo"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuGhUuNIE47a",
    "outputId": "d7db1411-1ad4-4954-ba1a-094c09c0ce0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf logs/train\n",
    "!rm -rf logs/val\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "name = \"tf_auctions_40M\"\n",
    "\n",
    "logger = TensorBoardLogger( # tensorboard --logdir=notebooks/logs\n",
    "    save_dir=\"logs\",\n",
    "    name=name,\n",
    "    version='full_train-128b-wpos-ch-weighted'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='../models/auction_transformer_40M_128b_wpos_ch_weighted',\n",
    "    filename='epoch_{epoch:02d}',\n",
    "    save_top_k=-1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    logger=logger,\n",
    "    limit_val_batches=500,\n",
    "    val_check_interval=0.25,  # Validate every 25% of training epoch\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit on single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf logs/train\n",
    "!rm -rf logs/val\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "name = \"tf_auctions_2.0M\"\n",
    "\n",
    "logger = TensorBoardLogger( # tensorboard --logdir=logs\n",
    "    save_dir=\"logs\",\n",
    "    name=name,\n",
    "    version='overfit'\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1000,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=1,\n",
    "    logger=logger,\n",
    "    limit_train_batches=1,  # Overfit on single batch\n",
    "    limit_val_batches=1,\n",
    "    val_check_interval=1,\n",
    "    overfit_batches=1\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
