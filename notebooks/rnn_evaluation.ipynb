{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "wd = Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from prediction_engine.model import AuctionPredictor\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('../data/items.csv')\n",
    "print(\"Items shape:\", items.shape)\n",
    "\n",
    "n_items = len(items)\n",
    "item_to_index = {item_id: i + 2 for i, item_id in enumerate(items['item_id'])}\n",
    "item_to_index[0] = 0 \n",
    "item_to_index[1] = 1  \n",
    "print(f\"Number of unique items: {n_items}\")\n",
    "\n",
    "time_left_mapping = {\n",
    "    'VERY_LONG': 48,\n",
    "    'LONG': 12,\n",
    "    'MEDIUM': 2,\n",
    "    'SHORT': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_auctions_from_sample(data_dir):\n",
    "    file_info = {}\n",
    "    auction_appearances = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for filename in tqdm(files):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            date = datetime.strptime(filename.split('.')[0], '%Y%m%dT%H')\n",
    "            file_info[filepath] = date\n",
    "\n",
    "    file_info = {k: v for k, v in sorted(file_info.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    all_auctions = []\n",
    "    \n",
    "    for filepath in list(file_info.keys()):\n",
    "        with open(filepath, 'r') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                \n",
    "                if 'auctions' not in json_data:\n",
    "                    print(f\"File {filepath} does not contain 'auctions' key, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                auction_data = json_data['auctions']\n",
    "                timestamp = file_info[filepath]\n",
    "                \n",
    "                for auction in auction_data:\n",
    "                    auction_id = auction['id']\n",
    "                    auction['timestamp'] = timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    \n",
    "                    if auction_id not in auction_appearances:\n",
    "                        auction_appearances[auction_id] = {'first': timestamp, 'last': timestamp}\n",
    "                    else:\n",
    "                        auction_appearances[auction_id]['last'] = timestamp\n",
    "                \n",
    "                all_auctions.extend(auction_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return all_auctions, auction_appearances\n",
    "\n",
    "def process_auction_data(auctions, auction_appearances, prediction_time):\n",
    "    auctions_by_item = {}\n",
    "    hours_on_sale = {}\n",
    "    auction_ids_by_item = {}\n",
    "    hours_since_first_appearance_values = [] \n",
    "\n",
    "    for auction in auctions:\n",
    "        auction_id = auction['id']\n",
    "        item_id = auction['item']['id']\n",
    "        time_left_numeric = time_left_mapping.get(auction['time_left'], 0) / 48.0\n",
    "        bid = np.log1p(auction['bid'] / 10000.0) / 15.0\n",
    "        buyout = np.log1p(auction['buyout'] / 10000.0) / 15.0\n",
    "        quantity = auction['quantity'] / 200.0\n",
    "        item_index = item_to_index.get(item_id, 1)\n",
    "        timestamp = datetime.strptime(auction['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        if timestamp != prediction_time:\n",
    "            continue\n",
    "\n",
    "        hours_since_first_appearance = (prediction_time - auction_appearances[auction_id]['first']).total_seconds() / 3600\n",
    "        hours_since_first_appearance_values.append(hours_since_first_appearance)  \n",
    "        hours_since_first_appearance_normalized = hours_since_first_appearance / 48.0\n",
    "        hours_on_sale[auction_id] = (auction_appearances[auction_id]['last'] - prediction_time).total_seconds() / 3600\n",
    "\n",
    "        datetime_str = prediction_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        processed_auction = [\n",
    "            bid, \n",
    "            buyout,  \n",
    "            quantity, \n",
    "            item_index,\n",
    "            time_left_numeric, \n",
    "            hours_since_first_appearance_normalized\n",
    "        ]\n",
    "        \n",
    "        if item_index not in auctions_by_item:\n",
    "            auctions_by_item[item_index] = []\n",
    "            auction_ids_by_item[item_index] = []\n",
    "\n",
    "        auctions_by_item[item_index].append(processed_auction)\n",
    "        auction_ids_by_item[item_index].append(auction_id)\n",
    "\n",
    "    if hours_since_first_appearance_values:\n",
    "        print(f\"Hours since first appearance statistics: Min: {min(hours_since_first_appearance_values)}, Max: {max(hours_since_first_appearance_values)}, Mean: {np.mean(hours_since_first_appearance_values)}\")\n",
    "    \n",
    "    return auctions_by_item, auction_ids_by_item, hours_on_sale\n",
    "\n",
    "prediction_time = datetime.strptime(\"2024-10-12 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "data_dir = '../data/sample/'\n",
    "auction_data, auction_appearances = load_auctions_from_sample(data_dir)\n",
    "auctions_by_item, auction_ids_by_item, hours_on_sale = process_auction_data(auction_data, auction_appearances, prediction_time)\n",
    "print(f\"Processed auctions for {len(auctions_by_item)} different items.\")\n",
    "print(f\"Example of processed auctions for an item: {auctions_by_item[list(auctions_by_item.keys())[0]][0]}\")\n",
    "print(f\"Example of hours_on_sale for an auction: {list(hours_on_sale.items())[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AuctionPredictor(\n",
    "    n_items=len(item_to_index),             \n",
    "    input_size=5,                   \n",
    "    encoder_hidden_size=1024,\n",
    "    decoder_hidden_size=1024,\n",
    "    item_index=3,                   \n",
    "    embedding_size=512,\n",
    "    dropout_p=0.2,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "model_path = '../models/checkpoint_epoch_1_iter_10336.pt'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  \n",
    "print('Pre-trained RNN model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn_with_inspection_and_mae(model, auctions_by_item, auction_ids_by_item, hours_on_sale, prediction_time):\n",
    "    all_predictions = []\n",
    "    all_actual_values = []\n",
    "\n",
    "    print(f\"Total number of items: {len(auctions_by_item)}\")\n",
    "    print(f\"Total number of auctions: {sum(len(auctions) for auctions in auctions_by_item.values())}\")\n",
    "\n",
    "    if hours_on_sale:\n",
    "        print(f\"Hours on sale statistics: Min: {min(hours_on_sale.values())}, Max: {max(hours_on_sale.values())}, Mean: {np.mean(list(hours_on_sale.values()))}\")\n",
    "    else:\n",
    "        print(\"No 'hours_on_sale' data to calculate statistics.\")\n",
    "\n",
    "    for item_idx, auctions in auctions_by_item.items():\n",
    "        if not auctions:\n",
    "            continue\n",
    "        \n",
    "        auctions_np = np.array(auctions)\n",
    "        X = torch.tensor(auctions_np, dtype=torch.float32).to(device)\n",
    "        X = X.unsqueeze(0)\n",
    "        lengths = torch.tensor([X.size(1)], dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(X, lengths)\n",
    "\n",
    "        auction_ids = auction_ids_by_item.get(item_idx, [])  \n",
    "        actual_values = [hours_on_sale.get(auction_id, 0) for auction_id in auction_ids]  \n",
    "\n",
    "        if len(predictions.squeeze(0)) == len(actual_values):\n",
    "            all_predictions.extend(predictions.squeeze((0, -1)).cpu().numpy())\n",
    "            all_actual_values.extend(actual_values)\n",
    "        else:\n",
    "            print(f\"Skipping item {item_idx} due to size mismatch: {len(predictions.squeeze(0))} predictions vs {len(actual_values)} actual values.\")\n",
    "\n",
    "    if not all_predictions:\n",
    "        print(\"No valid auctions were processed. Check your data.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Number of predictions: {len(all_predictions)}\")\n",
    "    print(f\"Number of actual values: {len(all_actual_values)}\")\n",
    "    print(f\"Predictions statistics: Min: {min(all_predictions)}, Max: {max(all_predictions)}, Mean: {np.mean(all_predictions)}\")\n",
    "    print(f\"Actual values statistics: Min: {min(all_actual_values)}, Max: {max(all_actual_values)}, Mean: {np.mean(all_actual_values)}\")\n",
    "\n",
    "    return all_predictions, all_actual_values\n",
    "\n",
    "def calculate_mae(all_predictions, all_actual_values):\n",
    "    if len(all_predictions) == 0 or len(all_actual_values) == 0:\n",
    "        print(\"No valid data for MAE calculation.\")\n",
    "        return None\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actual_values = np.array(all_actual_values)\n",
    "    \n",
    "    mae = mean_absolute_error(all_actual_values, all_predictions)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "all_predictions, all_actual_values = evaluate_rnn_with_inspection_and_mae(\n",
    "    model, auctions_by_item, auction_ids_by_item, hours_on_sale, prediction_time\n",
    ")\n",
    "\n",
    "if all_predictions is not None and all_actual_values is not None:\n",
    "    rnn_mae = calculate_mae(all_predictions, all_actual_values)\n",
    "    if rnn_mae is not None:\n",
    "        print(f'RNN Model MAE: {rnn_mae}')\n",
    "    else:\n",
    "        print('Evaluation failed due to lack of valid data.')\n",
    "else:\n",
    "    print('No predictions were made.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'target': all_actual_values,\n",
    "    'prediction': all_predictions\n",
    "})\n",
    "\n",
    "df['error'] = np.abs(df['target'] - df['prediction'])\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('target > 30').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df[['target', 'prediction']])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.target <= 10].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.target >= 30].error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.target >= 35].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../generated/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
