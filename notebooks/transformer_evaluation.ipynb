{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "wd = Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from src.models.auction_transformer import AuctionTransformer\n",
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_time = datetime.strptime(\"2025-03-20 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "mappings_dir = '../generated/mappings'\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'item_to_idx.json'), 'r') as f:\n",
    "        item_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'context_to_idx.json'), 'r') as f:\n",
    "    context_to_idx = json.load(f)\n",
    "    \n",
    "with open(os.path.join(mappings_dir, 'bonus_to_idx.json'), 'r') as f:\n",
    "    bonus_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'modtype_to_idx.json'), 'r') as f:\n",
    "    modtype_to_idx = json.load(f)\n",
    "\n",
    "feature_stats = torch.load('../generated/feature_stats.pt')\n",
    "\n",
    "time_left_mapping = {\n",
    "    'VERY_LONG': 48,\n",
    "    'LONG': 12,\n",
    "    'MEDIUM': 2,\n",
    "    'SHORT': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_auctions_from_sample(data_dir, prediction_time):\n",
    "    file_info = {}\n",
    "    auction_appearances = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for filename in tqdm(files):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            date = datetime.strptime(filename.split('.')[0], '%Y%m%dT%H')\n",
    "            file_info[filepath] = date\n",
    "\n",
    "    file_info = {k: v for k, v in sorted(file_info.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    raw_auctions = []\n",
    "    \n",
    "    for filepath in list(file_info.keys()):\n",
    "        with open(filepath, 'r') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                \n",
    "                if 'auctions' not in json_data:\n",
    "                    print(f\"File {filepath} does not contain 'auctions' key, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                auction_data = json_data['auctions']\n",
    "                timestamp = file_info[filepath]\n",
    "                \n",
    "                for auction in auction_data:\n",
    "                    auction_id = auction['id']\n",
    "\n",
    "                    if auction_id not in auction_appearances:\n",
    "                        auction_appearances[auction_id] = {'first': timestamp, 'last': timestamp}\n",
    "                    else:\n",
    "                        auction_appearances[auction_id]['last'] = timestamp\n",
    "                \n",
    "                if prediction_time is not None:\n",
    "                    if timestamp == prediction_time:\n",
    "                        raw_auctions.extend(auction_data)\n",
    "                else:\n",
    "                    raw_auctions.extend(auction_data)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "    auctions = []\n",
    "    for auction in tqdm(raw_auctions):\n",
    "        first_appearance = auction_appearances[auction['id']]['first']\n",
    "        last_appearance = auction_appearances[auction['id']]['last']\n",
    "\n",
    "        auction_id = auction['id']\n",
    "        item_id = auction['item']['id']\n",
    "        item_index = item_to_idx[str(item_id)]\n",
    "        bid = auction.get('bid', 0) / 10000.0\n",
    "        buyout = auction['buyout'] / 10000.0\n",
    "        quantity = auction['quantity']\n",
    "        time_left = time_left_mapping[auction['time_left']]\n",
    "        context = context_to_idx[str(auction['item'].get('context', 0))]\n",
    "        bonus_lists = [bonus_to_idx[str(bonus)] for bonus in auction['item'].get('bonus_lists', [])]\n",
    "        modifiers = auction['item'].get('modifiers', [])\n",
    "\n",
    "        modifier_types = []\n",
    "        modifier_values = []\n",
    "\n",
    "        for modifier in modifiers:\n",
    "            modifier_types.append(modtype_to_idx[str(modifier['type'])])\n",
    "            modifier_values.append(modifier['value'])\n",
    "\n",
    "        if 'pet_species_id' in auction['item']:\n",
    "            continue\n",
    "\n",
    "        first_appearance = first_appearance.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        last_appearance = last_appearance.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        auctions.append([\n",
    "            auction_id,\n",
    "            item_index,\n",
    "            bid,\n",
    "            buyout,\n",
    "            quantity,\n",
    "            time_left,\n",
    "            context,\n",
    "            bonus_lists,\n",
    "            modifier_types,\n",
    "            modifier_values,\n",
    "            first_appearance,\n",
    "            last_appearance\n",
    "        ])\n",
    "        \n",
    "    df_auctions = pd.DataFrame(auctions, columns=['id', 'item_index', 'bid', 'buyout', 'quantity', 'time_left', 'context', 'bonus_lists', 'modifier_types', 'modifier_values', 'first_appearance', 'last_appearance'])\n",
    "    df_auctions['first_appearance'] = pd.to_datetime(df_auctions['first_appearance'])\n",
    "    df_auctions['last_appearance'] = pd.to_datetime(df_auctions['last_appearance'])\n",
    "\n",
    "    df_auctions = df_auctions[(df_auctions['first_appearance'] <= prediction_time) & (df_auctions['last_appearance'] >= prediction_time)]\n",
    "\n",
    "    df_auctions['current_hours'] = (prediction_time - df_auctions['first_appearance']).dt.total_seconds() / 3600\n",
    "    df_auctions['hours_on_sale'] = (df_auctions['last_appearance'] - prediction_time).dt.total_seconds() / 3600\n",
    "\n",
    "    return df_auctions\n",
    "\n",
    "data_dir = '../data/sample/'\n",
    "\n",
    "df_auctions = load_auctions_from_sample(data_dir, prediction_time)\n",
    "\n",
    "print(\"Auctions shape:\", df_auctions.shape)\n",
    "df_auctions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions[['current_hours', 'hours_on_sale']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AuctionTransformer.load_from_checkpoint(\n",
    "    '../models/auction_transformer_7.2M_128b_wpos_ch_filtered/last-v1.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "model.eval()\n",
    "print('Pre-trained Transformer model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "model = model.to('cuda')\n",
    "df_auctions = predict_dataframe(model, df_auctions, prediction_time, feature_stats)\n",
    "\n",
    "print(\"Mean hours on sale:\", df_auctions['hours_on_sale'].mean())\n",
    "print(\"Mean prediction:\", df_auctions['prediction'].mean())\n",
    "print(\"Mean sale probability:\", df_auctions['sale_probability'].mean())\n",
    "\n",
    "df_auctions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_12h = df_auctions[df_auctions['current_hours'] <= 12]\n",
    "len(df_auctions_12h)\n",
    "\n",
    "mae = mean_absolute_error(df_auctions_12h['hours_on_sale'], df_auctions_12h['prediction'])\n",
    "print(f\"Mean absolute error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to excel\n",
    "df_auctions.to_excel('../generated/predictions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'item_name',\n",
    "    'bid',\n",
    "    'buyout',\n",
    "    'quantity',\n",
    "    'time_left',\n",
    "    'first_appearance',\n",
    "    'last_appearance',\n",
    "    'hours_since_first_appearance',\n",
    "    'hours_on_sale',\n",
    "    'prediction',\n",
    "    'sale_probability'\n",
    "]\n",
    "\n",
    "df_error = df_auctions[columns].copy()\n",
    "df_error['error'] = np.abs(df_error['hours_on_sale'] - df_error['prediction'])\n",
    "df_error['time_left'] = df_error['time_left'].map(time_left_mapping)\n",
    "\n",
    "df_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_error[['hours_on_sale', 'prediction']])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of hours on sale and prediction\n",
    "plt.hist(df_error['hours_on_sale'], bins=50, alpha=0.5, label='Hours on sale')\n",
    "plt.hist(df_error['prediction'], bins=50, alpha=0.5, label='Prediction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_error[(df_error['time_left'] == 48.0) & (df_error['hours_since_first_appearance'] < 12)]['hours_on_sale'], bins=50, alpha=0.5, label='Hours on sale')\n",
    "plt.hist(df_error[(df_error['time_left'] == 48.0) & (df_error['hours_since_first_appearance'] < 12)]['prediction'], bins=50, alpha=0.5, label='Prediction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_error['hours_since_first_appearance'], bins=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in evaluating the model when the items are recently published, because this will be the main use case for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df_error['hours_since_first_appearance'] <= 12) & (df_error['hours_on_sale'] <= 5) & (df_error['time_left'] == 48.0)\n",
    "query_df = df_error[query]\n",
    "print(f\"Mean sale probability: {query_df['sale_probability'].mean()}\")\n",
    "print(f\"Mean error: {query_df['error'].mean()}\")\n",
    "print(f\"Mean hours on sale: {query_df['hours_on_sale'].mean()}\")\n",
    "query_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df['hours_on_sale'].hist(bins=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_error[['bid', 'buyout', 'quantity', 'time_left', 'hours_since_first_appearance', 'sale_probability',\n",
    "                        'hours_on_sale', 'prediction', 'error']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm',\n",
    "            vmin=-1, vmax=1, \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True) \n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error.to_csv('../generated/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().parent.resolve()\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data.auction_dataset import AuctionDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.read_csv('../generated/auction_indices.csv')\n",
    "pairs = pairs[pairs['g_hours_on_sale_max'] < 50]\n",
    "pairs = pairs[pairs['g_current_hours_max'] < 50]\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.15, random_state=42, shuffle=False)\n",
    "\n",
    "print(f\"Before filtering: {len(train_pairs)}\")\n",
    "\n",
    "train_pairs = train_pairs[train_pairs['g_hours_on_sale_len'] <= 32]\n",
    "val_pairs = val_pairs[val_pairs['g_hours_on_sale_len'] <= 32]\n",
    "\n",
    "print(f\"After filtering: {len(train_pairs)}\\n\")\n",
    "\n",
    "train_pairs = train_pairs[:int(len(train_pairs)*0.9)]\n",
    "val_pairs = val_pairs[val_pairs['record'] < '2025-03-23']\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "print(f\"Val pairs: {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs = val_pairs[val_pairs['record'] == '2025-03-20 00:00:00']\n",
    "val_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "mappings_dir = '../generated/mappings'\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'item_to_idx.json'), 'r') as f:\n",
    "    item_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'context_to_idx.json'), 'r') as f:\n",
    "    context_to_idx = json.load(f)\n",
    "    \n",
    "with open(os.path.join(mappings_dir, 'bonus_to_idx.json'), 'r') as f:\n",
    "    bonus_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'modtype_to_idx.json'), 'r') as f:\n",
    "    modtype_to_idx = json.load(f)\n",
    "\n",
    "feature_stats = torch.load('../generated/feature_stats.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AuctionTransformer.load_from_checkpoint(\n",
    "    '../models/auction_transformer_7.2M_128b_wpos_ch_filtered/epoch_epoch=02.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "model.eval()\n",
    "print('Pre-trained Transformer model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 15431\n"
     ]
    }
   ],
   "source": [
    "from src.data.auction_dataset import AuctionDataset\n",
    "from src.data.utils import collate_auctions\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "val_dataset = AuctionDataset(val_pairs, feature_stats=feature_stats, path='../generated/sequences.h5')\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_auctions) \n",
    "\n",
    "a = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483/483 [00:20<00:00, 23.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.06371152400970459\n",
      "Validation MAE: 8.173335075378418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation set\n",
    "total_mse = 0\n",
    "total_mae = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        (auctions, item_index, contexts, bonus_lists, modifier_types, modifier_values, current_hours), y = batch\n",
    "\n",
    "        auctions = auctions.to(device)\n",
    "        item_index = item_index.to(device)\n",
    "        contexts = contexts.to(device)\n",
    "        bonus_lists = bonus_lists.to(device)\n",
    "        modifier_types = modifier_types.to(device)\n",
    "        modifier_values = modifier_values.to(device)\n",
    "        current_hours = current_hours.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #print(f'auctions {auctions[0]}')\n",
    "        #print(f'item_index {item_index[0]}')\n",
    "        #print(f'contexts {contexts[0]}')\n",
    "        #print(f'bonus_lists {bonus_lists[0]}')\n",
    "        #print(f'modifier_types {modifier_types[0]}')\n",
    "        #print(f'modifier_values {modifier_values[0]}')\n",
    "\n",
    "        model.eval()\n",
    "        y_hat = model((auctions, item_index, contexts, bonus_lists, modifier_types, modifier_values))\n",
    "        \n",
    "        mask = (item_index != 0).float().unsqueeze(-1)\n",
    "        current_hours_mask = (current_hours <= 12.0).float().unsqueeze(-1)\n",
    "        mask = mask * current_hours_mask\n",
    "        #print(item_index[0])\n",
    "        #print(current_hours[0])\n",
    "        \n",
    "        mse = model.criterion(y_hat * mask, y.unsqueeze(2) * mask) / mask.sum()\n",
    "        mae = torch.nn.functional.l1_loss(\n",
    "            y_hat * mask * 48.0,\n",
    "            y.unsqueeze(2) * mask * 48.0,\n",
    "            reduction='sum'\n",
    "        ) / mask.sum()\n",
    "\n",
    "        #print((y_hat* 48.0 * mask).squeeze(2)[0])\n",
    "        #print((y.unsqueeze(2) * mask * 48.0).squeeze(2)[0])\n",
    "        \n",
    "        total_mse += mse.item() * mask.sum()\n",
    "        total_mae += mae.item() * mask.sum()\n",
    "        total_samples += mask.sum()\n",
    "\n",
    "        #break\n",
    "\n",
    "avg_mse = total_mse / total_samples\n",
    "avg_mae = total_mae / total_samples\n",
    "print(f'Validation MSE: {avg_mse}')\n",
    "print(f'Validation MAE: {avg_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions[df_auctions['item_index'] == 13815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "predict_dataframe(model, df_auctions[df_auctions['item_index'] == 13815], prediction_time, feature_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 1.0614372e+05 1.0000000e+00 4.8000000e+01 2.5000000e+01\n",
      "  2.2000000e+01]\n",
      " [1.9588410e+04 3.6245220e+04 1.0000000e+00 4.8000000e+01 2.2000000e+01\n",
      "  2.5000000e+01]\n",
      " [0.0000000e+00 3.6245220e+04 1.0000000e+00 4.8000000e+01 6.0000000e+00\n",
      "  1.7000000e+01]]\n",
      "[]\n",
      "[0 0 0]\n",
      "[[4.]\n",
      " [4.]\n",
      " [4.]]\n",
      "[[1735.]\n",
      " [1735.]\n",
      " [1735.]]\n"
     ]
    }
   ],
   "source": [
    "# open the file ../data/generated_test/sequences.h5 and print the auctions for item_index 2025-03-20/00/13815/auctions\n",
    "with h5py.File('../generated/sequences.h5', 'r') as f:\n",
    "    print(f['2025-03-20/00/7139/auctions'][:]) # there should be 8 auctions\n",
    "    print(f['2025-03-20/00/7139/bonus_lists'][:])\n",
    "    print(f['2025-03-20/00/7139/contexts'][:])\n",
    "    print(f['2025-03-20/00/7139/modifier_types'][:])\n",
    "    print(f['2025-03-20/00/7139/modifier_values'][:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
