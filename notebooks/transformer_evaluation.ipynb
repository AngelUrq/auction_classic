{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "wd = Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from src.models.auction_transformer import AuctionTransformer\n",
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_time = datetime.strptime(\"2025-11-01 02:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "max_hours_back = 24\n",
    "\n",
    "mappings_dir = '../generated/mappings'\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'item_to_idx.json'), 'r') as f:\n",
    "        item_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'context_to_idx.json'), 'r') as f:\n",
    "    context_to_idx = json.load(f)\n",
    "    \n",
    "with open(os.path.join(mappings_dir, 'bonus_to_idx.json'), 'r') as f:\n",
    "    bonus_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'modtype_to_idx.json'), 'r') as f:\n",
    "    modtype_to_idx = json.load(f)\n",
    "\n",
    "feature_stats = torch.load('../generated/feature_stats.pt')\n",
    "\n",
    "time_left_mapping = {\n",
    "    'VERY_LONG': 48,\n",
    "    'LONG': 12,\n",
    "    'MEDIUM': 2,\n",
    "    'SHORT': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass 1/2: appearances: 100%|██████████| 142/142 [00:48<00:00,  2.92it/s]\n",
      "Pass 2/2: rows: 100%|██████████| 142/142 [02:31<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataframe with 11885740 rows from 142 snapshots [2025-10-29 02:00, 2025-11-04 02:00], include_targets=True\n",
      "Auctions shape: (11885740, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_index</th>\n",
       "      <th>bid</th>\n",
       "      <th>buyout</th>\n",
       "      <th>quantity</th>\n",
       "      <th>time_left</th>\n",
       "      <th>context</th>\n",
       "      <th>bonus_lists</th>\n",
       "      <th>modifier_types</th>\n",
       "      <th>modifier_values</th>\n",
       "      <th>snapshot_time</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>hour_of_week</th>\n",
       "      <th>current_hours</th>\n",
       "      <th>hours_on_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5884501</th>\n",
       "      <td>313018714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6</td>\n",
       "      <td>[1269, 834, 1, 1238, 1, 347, 1212]</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>[2462, 40, 49]</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884502</th>\n",
       "      <td>313026277</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350000.69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6</td>\n",
       "      <td>[1269, 451, 834, 1255, 1252, 1238, 1, 347, 1212]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2462]</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884503</th>\n",
       "      <td>313028728</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1300.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>61</td>\n",
       "      <td>[1, 834, 1269, 1]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2462]</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884504</th>\n",
       "      <td>313029050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>825.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>61</td>\n",
       "      <td>[1, 834, 1, 1269, 1]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2462]</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884505</th>\n",
       "      <td>313029399</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1995.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>[4, 8, 9, 11]</td>\n",
       "      <td>[2734, 54185, 4744, 340]</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  item_index  bid     buyout  quantity  time_left  context  \\\n",
       "5884501  313018714           1  0.0  150000.00         1        0.5        6   \n",
       "5884502  313026277           1  0.0  350000.69         1        0.5        6   \n",
       "5884503  313028728           1  0.0    1300.68         1        0.5       61   \n",
       "5884504  313029050           1  0.0     825.00         1        0.5       61   \n",
       "5884505  313029399           1  0.0    1995.00         1        0.5       12   \n",
       "\n",
       "                                              bonus_lists modifier_types  \\\n",
       "5884501                [1269, 834, 1, 1238, 1, 347, 1212]      [4, 5, 6]   \n",
       "5884502  [1269, 451, 834, 1255, 1252, 1238, 1, 347, 1212]            [4]   \n",
       "5884503                                 [1, 834, 1269, 1]            [4]   \n",
       "5884504                              [1, 834, 1, 1269, 1]            [4]   \n",
       "5884505                                                []  [4, 8, 9, 11]   \n",
       "\n",
       "                  modifier_values       snapshot_time  time_offset  \\\n",
       "5884501            [2462, 40, 49] 2025-11-01 02:00:00            0   \n",
       "5884502                    [2462] 2025-11-01 02:00:00            0   \n",
       "5884503                    [2462] 2025-11-01 02:00:00            0   \n",
       "5884504                    [2462] 2025-11-01 02:00:00            0   \n",
       "5884505  [2734, 54185, 4744, 340] 2025-11-01 02:00:00            0   \n",
       "\n",
       "         hour_of_week  current_hours  hours_on_sale  \n",
       "5884501           122           47.0            0.0  \n",
       "5884502           122           47.0            0.0  \n",
       "5884503           122           47.0            0.0  \n",
       "5884504           122           47.0            0.0  \n",
       "5884505           122           47.0            0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.utils import load_auctions_from_sample\n",
    "\n",
    "data_dir = '../data/tww/auctions_ch/'\n",
    "\n",
    "df_auctions = load_auctions_from_sample(data_dir, prediction_time, time_left_mapping, item_to_idx, context_to_idx, bonus_to_idx, modtype_to_idx, max_hours_back=max_hours_back)\n",
    "\n",
    "print(\"Auctions shape:\", df_auctions.shape)\n",
    "df_auctions[df_auctions['snapshot_time'] == prediction_time].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 868035\n",
      "Pre-trained Transformer model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = AuctionTransformer.load_from_checkpoint(\n",
    "    '../models/transformer-868.0K-quantile-historical_24/last.ckpt',\n",
    "    #'../models/transformer-869.6K-quantile-historical_72/last.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "model.eval()\n",
    "print('Pre-trained Transformer model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "model = model.to('cuda')\n",
    "df_auctions = predict_dataframe(model, df_auctions, prediction_time, feature_stats, max_hours_back=max_hours_back, max_sequence_length=55000)\n",
    "\n",
    "df_predictions = df_auctions[df_auctions['time_offset'] == 0]\n",
    "print(f\"Number of rows with nan prediction_q50: {df_predictions['prediction_q50'].isna().sum()}\")\n",
    "df_predictions = df_predictions[df_predictions['prediction_q50'].notna()]\n",
    "\n",
    "print(\"Mean hours on sale:\", df_predictions['hours_on_sale'].mean())\n",
    "print(\"Mean prediction:\", df_predictions['prediction_q50'].mean())\n",
    "\n",
    "mae = mean_absolute_error(df_predictions['hours_on_sale'], df_predictions['prediction_q50'])\n",
    "print(f\"Mean absolute error: {mae}\") # 3.14 -> 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_filtered = df_predictions[df_predictions['current_hours'] == 0]\n",
    "df_auctions_filtered = df_auctions_filtered[df_auctions_filtered['time_left'] == 48.0]\n",
    "\n",
    "df_auctions_filtered['prediction_interval'] = df_auctions_filtered['prediction_q90'] - df_auctions_filtered['prediction_q10']\n",
    "\n",
    "mae = mean_absolute_error(df_auctions_filtered['hours_on_sale'], df_auctions_filtered['prediction_q50'])\n",
    "print(f\"Mean absolute error: {mae} for {len(df_auctions_filtered)} auctions\") # 6.54 -> 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_filtered[['item_index', 'buyout','quantity', 'time_left', 'current_hours', 'hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_filtered['coverage'] = (df_auctions_filtered['prediction_q10'] <= df_auctions_filtered['hours_on_sale']) & (df_auctions_filtered['hours_on_sale'] <= df_auctions_filtered['prediction_q90'])\n",
    "df_auctions_filtered['coverage'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "df_auctions_filtered = df_predictions[df_predictions['current_hours'] == 0]\n",
    "df_auctions_filtered = df_auctions_filtered[df_auctions_filtered['time_left'] == 48.0]\n",
    "\n",
    "df_auctions_filtered['sold_gt'] = df_auctions_filtered['hours_on_sale'] <= 12\n",
    "\n",
    "df_auctions_filtered['sold_pred'] = (df_auctions_filtered['prediction_q50'] <= 12.0) & (df_auctions_filtered['prediction_q90'] <= 20.0)\n",
    "\n",
    "num_positive = df_auctions_filtered['sold_gt'].sum()\n",
    "num_negative = len(df_auctions_filtered) - num_positive\n",
    "\n",
    "print(f\"Total samples: {len(df_auctions_filtered)}\")\n",
    "print(f\"Positive (sold ≤ hours): {num_positive}\")\n",
    "print(f\"Negative (not sold > hours): {num_negative}\")\n",
    "print(f\"Predicted positive: {df_auctions_filtered['sold_pred'].sum()}\")\n",
    "print(f\"Predicted negative: {len(df_auctions_filtered) - df_auctions_filtered['sold_pred'].sum()}\")\n",
    "print(f\"Class balance: {num_positive / len(df_auctions_filtered):.2%} positives\")\n",
    "\n",
    "accuracy = accuracy_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "precision = precision_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "recall = recall_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "f1 = f1_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision} (when it claims it will sell, it is right in {precision:.2%} of the cases)\")\n",
    "print(f\"Recall: {recall} (of the ones that will sell, it catches {recall:.2%} of them)\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_true = df_auctions_filtered['sold_gt'].astype(int)\n",
    "y_score = df_auctions_filtered['prediction_q50']\n",
    "\n",
    "# Sweep thresholds from 0 to 48 hours\n",
    "thresholds = np.linspace(0, 48, 200)\n",
    "\n",
    "precisions = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_score <= t)\n",
    "    \n",
    "    # Avoid undefined precision (no positive predictions)\n",
    "    if y_pred.sum() == 0:\n",
    "        precisions.append(np.nan)\n",
    "    else:\n",
    "        precisions.append(precision_score(y_true, y_pred))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, precisions)\n",
    "plt.xlabel(\"Threshold on prediction_q50 (hours)\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Threshold\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Sold\", \"Sold\"])\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'item_index',\n",
    "    'bid',\n",
    "    'buyout',\n",
    "    'quantity',\n",
    "    'time_left',\n",
    "    'current_hours',\n",
    "    'hours_on_sale',\n",
    "    'prediction_q10',\n",
    "    'prediction_q50',\n",
    "    'prediction_q90',\n",
    "    'prediction_interval',\n",
    "    'sale_probability'\n",
    "]\n",
    "\n",
    "df_error = df_auctions_filtered[columns].copy()\n",
    "df_error['error'] = np.abs(df_error['hours_on_sale'] - df_error['prediction_q50'])\n",
    "\n",
    "df_error.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error['time_left'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_error[['hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90']])\n",
    "plt.xticks(ticks=[1,2,3,4], labels=['hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins for hours_on_sale\n",
    "bins = [(0,12), (12,24), (24,48)]\n",
    "\n",
    "# Calculate mean error for each bin\n",
    "for start, end in bins:\n",
    "    mask = (df_error['hours_on_sale'] >= start) & (df_error['hours_on_sale'] <= end)\n",
    "    mean_error = df_error[mask]['error'].mean()\n",
    "    print(f\"Mean error for hours {start}-{end}: {mean_error:.2f}\")\n",
    "\n",
    "# Create boxplot showing error distribution in each bin\n",
    "error_by_bin = []\n",
    "labels = []\n",
    "for start, end in bins:\n",
    "    mask = (df_error['hours_on_sale'] >= start) & (df_error['hours_on_sale'] <= end)\n",
    "    error_by_bin.append(df_error[mask]['error'])\n",
    "    labels.append(f\"{start}-{end}h\")\n",
    "\n",
    "plt.boxplot(error_by_bin, labels=labels)\n",
    "plt.title(\"Error Distribution by Hours on Sale\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.xlabel(\"Hours on Sale Range\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of hours on sale and prediction\n",
    "plt.hist(df_error['hours_on_sale'], bins=100, alpha=0.5, label='Hours on sale')\n",
    "plt.hist(df_error['prediction_q50'], bins=100, alpha=0.5, label='Prediction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_error['current_hours'], bins=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in evaluating the model when the items are recently published, because this will be the main use case for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df_error['current_hours'] <= 12) & (df_error['time_left'] == 48.0)\n",
    "query_df = df_error[query]\n",
    "print(f\"Mean sale probability: {query_df['sale_probability'].mean()}\")\n",
    "print(f\"Mean error: {query_df['error'].mean()}\")\n",
    "print(f\"Mean hours on sale: {query_df['hours_on_sale'].mean()}\")\n",
    "query_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df['hours_on_sale'].hist(bins=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_error[['bid', 'buyout', 'time_left', 'current_hours', 'sale_probability',\n",
    "                        'hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90', 'prediction_interval', 'error']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm',\n",
    "            vmin=-1, vmax=1, \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True) \n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().parent.resolve()\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data.auction_dataset import AuctionDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "    (\"g_hours_on_sale_max\", \"<=\", 50),\n",
    "    (\"g_current_hours_max\", \"<=\", 50),\n",
    "    (\"g_hours_on_sale_len\", \"<=\", 64),\n",
    "    (\"record\", \">=\", \"2025-07-01\"),\n",
    "    (\"record\", \"<=\", \"2025-09-01\"),\n",
    "]\n",
    "\n",
    "pairs = pd.read_parquet(\"../generated/indices.parquet\", engine=\"pyarrow\", filters=filters)\n",
    "\n",
    "split_idx = int(len(pairs) * 0.95)\n",
    "\n",
    "train_pairs = pairs.iloc[:split_idx]\n",
    "train_pairs = train_pairs.iloc[: int(len(train_pairs) * 0.90)]\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "\n",
    "val_pairs = pairs.iloc[split_idx:]\n",
    "\n",
    "print(f\"Val pairs: {len(val_pairs)}\")\n",
    "\n",
    "del pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.auction_dataset import AuctionDataset\n",
    "from src.data.utils import collate_auctions\n",
    "\n",
    "batch_size = 16\n",
    "max_hours_back = 24\n",
    "\n",
    "val_dataset = AuctionDataset(val_pairs, feature_stats=feature_stats, path='../generated/sequences.h5', max_hours_back=max_hours_back)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_auctions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_and_noise.py\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Helper: build a hashable signature of everything the network can actually\n",
    "#    distinguish once log1p + norm have been undone.\n",
    "#    (Same fields as before; model now also sees hour_of_week/time_offset, but\n",
    "#     we keep the signature aligned with original invariants.)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def auction_signature(item_idx,\n",
    "                      quantity,\n",
    "                      buyout_gold,\n",
    "                      context,\n",
    "                      time_left,\n",
    "                      current_hours,\n",
    "                      bonus_lists,\n",
    "                      modifier_types,\n",
    "                      age_bucket=1.0):  # 1-hour buckets\n",
    "    price_sig = round(float(buyout_gold), 2)   # 0.01 g precision\n",
    "    age_sig   = int(current_hours // age_bucket)\n",
    "\n",
    "    return (\n",
    "        int(item_idx),\n",
    "        int(quantity),\n",
    "        price_sig,\n",
    "        int(context),\n",
    "        int(time_left),\n",
    "        age_sig,\n",
    "        tuple(sorted(bonus_lists)),\n",
    "        tuple(sorted(modifier_types)),\n",
    "    )\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Validation + noise-floor loop (updated for new dataloader/model)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_with_noise(model, val_loader, feature_stats, max_val_batches=10000, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args\n",
    "    ----\n",
    "    model : AuctionTransformer (quantile outputs)\n",
    "    val_loader : DataLoader yielding dict-batches from AuctionDataset\n",
    "    feature_stats : {'means': tensor(5,), 'stds': tensor(5,), ...}\n",
    "    device : str\n",
    "    require_full_duration : bool\n",
    "        If True, restrict metrics/noise-floor to time_left==48.0 (like before).\n",
    "    \"\"\"\n",
    "\n",
    "    means = feature_stats[\"means\"].detach().cpu().numpy()   # first 5 auction cols\n",
    "    stds  = feature_stats[\"stds\"].detach().cpu().numpy()\n",
    "\n",
    "    total_mse   = 0.0\n",
    "    total_mae   = 0.0\n",
    "    total_items = 0.0\n",
    "\n",
    "    targets_by_sig = defaultdict(list)\n",
    "\n",
    "    # median channel = tau=0.5\n",
    "    # Try to find it; fallback to middle channel if needed.\n",
    "    if hasattr(model, \"quantiles\") and 0.5 in model.quantiles:\n",
    "        median_idx = model.quantiles.index(0.5)\n",
    "    else:\n",
    "        # Fallback: assume 3 quantiles [0.1,0.5,0.9]\n",
    "        median_idx = 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            if i > max_val_batches:\n",
    "                break\n",
    "            i += 1\n",
    "            # ---- move to device ------------------------------------------------\n",
    "            auctions        = batch['auctions'].to(device)           # (B,S,5) z-scored; bid/buyout were log1p pre-norm\n",
    "            item_index      = batch['item_index'].to(device)         # (B,S)\n",
    "            contexts        = batch['contexts'].to(device)           # (B,S)\n",
    "            bonus_lists     = batch['bonus_lists'].to(device)        # (B,S,K)\n",
    "            modifier_types  = batch['modifier_types'].to(device)     # (B,S,M)\n",
    "            modifier_values = batch['modifier_values'].to(device)    # (B,S,M) already log1p and normed\n",
    "            hour_of_week    = batch['hour_of_week'].to(device)       # (B,S)\n",
    "            time_offset     = batch['time_offset'].to(device)        # (B,S)\n",
    "            y               = batch['target'].to(device)             # (B,S) in HOURS (0..48)\n",
    "\n",
    "            current_hours_raw = batch['current_hours_raw'].to(device)  # (B,S)\n",
    "            time_left_raw     = batch['time_left_raw'].to(device)      # (B,S)\n",
    "\n",
    "            # ---- forward -------------------------------------------------------\n",
    "            y_hat_all = model((\n",
    "                auctions, item_index, contexts, bonus_lists,\n",
    "                modifier_types, modifier_values, hour_of_week, time_offset\n",
    "            ))                                                      # (B,S,Q)\n",
    "\n",
    "            # Take median channel (hours)\n",
    "            y_hat = y_hat_all[..., median_idx]                      # (B,S)\n",
    "\n",
    "            # ---- masks ---------------------------------------------------------\n",
    "            # real items\n",
    "            mask = (item_index != 0)\n",
    "\n",
    "            # only current auctions (time_offset == 0)\n",
    "            mask = mask & (time_offset == 0)\n",
    "            mask = mask & (current_hours_raw == 0.0)\n",
    "            mask = mask & (time_left_raw == 48.0)\n",
    "\n",
    "            mask_f = mask.float()\n",
    "\n",
    "            # ---- losses/metrics (in HOURS) ------------------------------------\n",
    "            # MSE and MAE on masked entries\n",
    "            sq_err_sum = ((y_hat - y) ** 2 * mask_f).sum()\n",
    "            abs_err_sum = (torch.abs(y_hat - y) * mask_f).sum()\n",
    "            n_items = mask_f.sum().item()\n",
    "\n",
    "            total_mse   += sq_err_sum.item()\n",
    "            total_mae   += abs_err_sum.item()\n",
    "            total_items += n_items\n",
    "\n",
    "            # ---- collect targets for irreducible-noise estimate ----------------\n",
    "            # Work on CPU/numpy\n",
    "            y_cpu               = y.detach().cpu().numpy()\n",
    "            mask_cpu            = mask.detach().cpu().numpy()\n",
    "            item_idx_cpu        = item_index.detach().cpu().numpy()\n",
    "            auctions_cpu        = auctions.detach().cpu().numpy()       # z-scores\n",
    "            contexts_cpu        = contexts.detach().cpu().numpy()\n",
    "            time_left_cpu       = time_left_raw.detach().cpu().numpy()\n",
    "            current_hours_cpu   = current_hours_raw.detach().cpu().numpy()\n",
    "            bonus_lists_cpu     = bonus_lists.detach().cpu().numpy()\n",
    "            modifier_types_cpu  = modifier_types.detach().cpu().numpy()\n",
    "\n",
    "            B, S, _ = auctions_cpu.shape\n",
    "            for b in range(B):\n",
    "                for s in range(S):\n",
    "                    if not mask_cpu[b, s]:\n",
    "                        continue\n",
    "\n",
    "                    # ---- undo standardisation + log1p for BUYOUT ---------------\n",
    "                    buyout_norm = auctions_cpu[b, s, 1]\n",
    "                    log_buyout  = buyout_norm * stds[1] + means[1]\n",
    "                    buyout_gold = np.expm1(log_buyout)\n",
    "\n",
    "                    # quantity column is linear but standardised\n",
    "                    qty_norm = auctions_cpu[b, s, 2]\n",
    "                    quantity = int(round(qty_norm * stds[2] + means[2]))\n",
    "\n",
    "                    # build signature (strip zeros from K/M dims)\n",
    "                    sig = auction_signature(\n",
    "                        item_idx_cpu[b, s],\n",
    "                        quantity,\n",
    "                        buyout_gold,\n",
    "                        contexts_cpu[b, s],\n",
    "                        time_left_cpu[b, s],\n",
    "                        current_hours_cpu[b, s],\n",
    "                        bonus_lists_cpu[b, s][bonus_lists_cpu[b, s] != 0],\n",
    "                        modifier_types_cpu[b, s][modifier_types_cpu[b, s] != 0],\n",
    "                    )\n",
    "\n",
    "                    targets_by_sig[sig].append(float(y_cpu[b, s]))\n",
    "\n",
    "    # ── aggregate --------------------------------------------------------------\n",
    "    if total_items > 0:\n",
    "        avg_mse = total_mse / total_items\n",
    "        avg_mae = total_mae / total_items\n",
    "    else:\n",
    "        avg_mse = float('nan')\n",
    "        avg_mae = float('nan')\n",
    "\n",
    "    # Pairwise absolute diffs for signatures with ≥2 targets (Bayesian lower bound)\n",
    "    noise_sum   = 0.0\n",
    "    noise_count = 0\n",
    "    for t_list in targets_by_sig.values():\n",
    "        if len(t_list) < 2:\n",
    "            continue\n",
    "        for a, b in itertools.combinations(t_list, 2):\n",
    "            noise_sum   += abs(a - b)\n",
    "            noise_count += 1\n",
    "\n",
    "    irreducible_mae = noise_sum / noise_count if noise_count else 0.0\n",
    "\n",
    "    print(f\"Validation MSE          : {avg_mse:.4f} (hours^2)\")\n",
    "    print(f\"Validation MAE          : {avg_mae:.4f} hours\")\n",
    "    print(f\"Bayesian lower-bound MAE: {irreducible_mae:.4f} hours\")\n",
    "\n",
    "    return avg_mse, avg_mae, irreducible_mae\n",
    "\n",
    "\n",
    "# Example call (match your training device variable/name)\n",
    "evaluate_with_noise(model, val_dataloader, feature_stats, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "predict_dataframe(model, df_auctions[df_auctions['item_index'] == 13815], prediction_time, feature_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
