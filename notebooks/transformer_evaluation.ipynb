{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "wd = Path(os.path.dirname(os.path.abspath(\"__file__\"))).parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from src.models.auction_transformer import AuctionTransformer\n",
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_time = datetime.strptime(\"2025-11-04 02:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "max_hours_back = 24\n",
    "\n",
    "mappings_dir = '../generated/mappings'\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'item_to_idx.json'), 'r') as f:\n",
    "        item_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'context_to_idx.json'), 'r') as f:\n",
    "    context_to_idx = json.load(f)\n",
    "    \n",
    "with open(os.path.join(mappings_dir, 'bonus_to_idx.json'), 'r') as f:\n",
    "    bonus_to_idx = json.load(f)\n",
    "\n",
    "with open(os.path.join(mappings_dir, 'modtype_to_idx.json'), 'r') as f:\n",
    "    modtype_to_idx = json.load(f)\n",
    "\n",
    "feature_stats = torch.load('../generated/feature_stats.pt')\n",
    "\n",
    "time_left_mapping = {\n",
    "    'VERY_LONG': 48,\n",
    "    'LONG': 12,\n",
    "    'MEDIUM': 2,\n",
    "    'SHORT': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import load_auctions_from_sample\n",
    "\n",
    "data_dir = '../data/tww/auctions/'\n",
    "\n",
    "df_auctions = load_auctions_from_sample(data_dir, prediction_time, time_left_mapping, item_to_idx, context_to_idx, bonus_to_idx, modtype_to_idx, max_hours_back=max_hours_back)\n",
    "\n",
    "print(\"Auctions shape:\", df_auctions.shape)\n",
    "df_auctions[df_auctions['snapshot_time'] == prediction_time].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AuctionTransformer.load_from_checkpoint(\n",
    "    #'../models/transformer-4M-quantile-how-historical_0/last.ckpt',\n",
    "    #'../models/transformer-4M-quantile-how-historical/last-v2.ckpt',\n",
    "    '../models/transformer-4M-quantile-how-historical_24/last-v1.ckpt',\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "model.eval()\n",
    "print('Pre-trained Transformer model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "model = model.to('cuda')\n",
    "df_auctions = predict_dataframe(model, df_auctions, prediction_time, feature_stats, max_hours_back=max_hours_back)\n",
    "\n",
    "df_predictions = df_auctions[df_auctions['time_offset'] == 0]\n",
    "\n",
    "print(\"Mean hours on sale:\", df_predictions['hours_on_sale'].mean())\n",
    "print(\"Mean prediction:\", df_predictions['prediction_q50'].mean())\n",
    "\n",
    "mae = mean_absolute_error(df_predictions['hours_on_sale'], df_predictions['prediction_q50'])\n",
    "print(f\"Mean absolute error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_filtered = df_predictions[df_predictions['current_hours'] == 0]\n",
    "df_auctions_filtered = df_auctions_filtered[df_auctions_filtered['time_left'] == 12.0]\n",
    "\n",
    "df_auctions_filtered['prediction_interval'] = df_auctions_filtered['prediction_q90'] - df_auctions_filtered['prediction_q10']\n",
    "\n",
    "mae = mean_absolute_error(df_auctions_filtered['hours_on_sale'], df_auctions_filtered['prediction_q50'])\n",
    "print(f\"Mean absolute error: {mae} for {len(df_auctions_filtered)} auctions\") # 7.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_filtered[['item_index', 'buyout','quantity', 'time_left', 'current_hours', 'hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auctions_filtered['coverage'] = (df_auctions_filtered['prediction_q10'] <= df_auctions_filtered['hours_on_sale']) & (df_auctions_filtered['hours_on_sale'] <= df_auctions_filtered['prediction_q90'])\n",
    "df_auctions_filtered['coverage'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "df_auctions_filtered['sold_gt'] = df_auctions_filtered['hours_on_sale'] <= 6\n",
    "df_auctions_filtered['sold_pred'] = df_auctions_filtered['prediction_q50'] <= 6.0\n",
    "\n",
    "num_positive = df_auctions_filtered['sold_gt'].sum()\n",
    "num_negative = len(df_auctions_filtered) - num_positive\n",
    "\n",
    "print(f\"Total samples: {len(df_auctions_filtered)}\")\n",
    "print(f\"Positive (sold ≤ hours): {num_positive}\")\n",
    "print(f\"Negative (not sold > hours): {num_negative}\")\n",
    "print(f\"Class balance: {num_positive / len(df_auctions_filtered):.2%} positives\")\n",
    "\n",
    "accuracy = accuracy_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "precision = precision_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "recall = recall_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "f1 = f1_score(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_auctions_filtered['sold_gt'], df_auctions_filtered['sold_pred'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Sold\", \"Sold\"])\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'item_index',\n",
    "    'bid',\n",
    "    'buyout',\n",
    "    'quantity',\n",
    "    'time_left',\n",
    "    'current_hours',\n",
    "    'hours_on_sale',\n",
    "    'prediction_q10',\n",
    "    'prediction_q50',\n",
    "    'prediction_q90',\n",
    "    'prediction_interval',\n",
    "    'sale_probability'\n",
    "]\n",
    "\n",
    "df_error = df_auctions_filtered[columns].copy()\n",
    "df_error['error'] = np.abs(df_error['hours_on_sale'] - df_error['prediction_q50'])\n",
    "\n",
    "df_error.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error['time_left'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_error[['hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90']])\n",
    "plt.xticks(ticks=[1,2,3,4], labels=['hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins for hours_on_sale\n",
    "bins = [(0,12), (12,24), (24,48)]\n",
    "\n",
    "# Calculate mean error for each bin\n",
    "for start, end in bins:\n",
    "    mask = (df_error['hours_on_sale'] >= start) & (df_error['hours_on_sale'] <= end)\n",
    "    mean_error = df_error[mask]['error'].mean()\n",
    "    print(f\"Mean error for hours {start}-{end}: {mean_error:.2f}\")\n",
    "\n",
    "# Create boxplot showing error distribution in each bin\n",
    "error_by_bin = []\n",
    "labels = []\n",
    "for start, end in bins:\n",
    "    mask = (df_error['hours_on_sale'] >= start) & (df_error['hours_on_sale'] <= end)\n",
    "    error_by_bin.append(df_error[mask]['error'])\n",
    "    labels.append(f\"{start}-{end}h\")\n",
    "\n",
    "plt.boxplot(error_by_bin, labels=labels)\n",
    "plt.title(\"Error Distribution by Hours on Sale\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.xlabel(\"Hours on Sale Range\") \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of hours on sale and prediction\n",
    "plt.hist(df_error['hours_on_sale'], bins=100, alpha=0.5, label='Hours on sale')\n",
    "plt.hist(df_error['prediction_q50'], bins=100, alpha=0.5, label='Prediction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_error['current_hours'], bins=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in evaluating the model when the items are recently published, because this will be the main use case for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df_error['current_hours'] <= 12) & (df_error['time_left'] == 48.0)\n",
    "query_df = df_error[query]\n",
    "print(f\"Mean sale probability: {query_df['sale_probability'].mean()}\")\n",
    "print(f\"Mean error: {query_df['error'].mean()}\")\n",
    "print(f\"Mean hours on sale: {query_df['hours_on_sale'].mean()}\")\n",
    "query_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df['hours_on_sale'].hist(bins=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_error[['bid', 'buyout', 'time_left', 'current_hours', 'sale_probability',\n",
    "                        'hours_on_sale', 'prediction_q10', 'prediction_q50', 'prediction_q90', 'prediction_interval', 'error']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm',\n",
    "            vmin=-1, vmax=1, \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True) \n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().parent.resolve()\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data.auction_dataset import AuctionDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "    (\"g_hours_on_sale_max\", \"<=\", 50),\n",
    "    (\"g_current_hours_max\", \"<=\", 50),\n",
    "    (\"g_hours_on_sale_len\", \"<=\", 64),\n",
    "    (\"record\", \">=\", \"2025-07-01\"),\n",
    "    (\"record\", \"<=\", \"2025-09-01\"),\n",
    "]\n",
    "\n",
    "pairs = pd.read_parquet(\"../generated/indices.parquet\", engine=\"pyarrow\", filters=filters)\n",
    "\n",
    "split_idx = int(len(pairs) * 0.95)\n",
    "\n",
    "train_pairs = pairs.iloc[:split_idx]\n",
    "train_pairs = train_pairs.iloc[: int(len(train_pairs) * 0.90)]\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "\n",
    "val_pairs = pairs.iloc[split_idx:]\n",
    "\n",
    "print(f\"Val pairs: {len(val_pairs)}\")\n",
    "\n",
    "del pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.auction_dataset import AuctionDataset\n",
    "from src.data.utils import collate_auctions\n",
    "\n",
    "batch_size = 16\n",
    "max_hours_back = 24\n",
    "\n",
    "val_dataset = AuctionDataset(val_pairs, feature_stats=feature_stats, path='../generated/sequences.h5', max_hours_back=max_hours_back)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_auctions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_and_noise.py\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Helper: build a hashable signature of everything the network can actually\n",
    "#    distinguish once log1p + norm have been undone.\n",
    "#    (Same fields as before; model now also sees hour_of_week/time_offset, but\n",
    "#     we keep the signature aligned with original invariants.)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def auction_signature(item_idx,\n",
    "                      quantity,\n",
    "                      buyout_gold,\n",
    "                      context,\n",
    "                      time_left,\n",
    "                      current_hours,\n",
    "                      bonus_lists,\n",
    "                      modifier_types,\n",
    "                      age_bucket=1.0):  # 1-hour buckets\n",
    "    price_sig = round(float(buyout_gold), 2)   # 0.01 g precision\n",
    "    age_sig   = int(current_hours // age_bucket)\n",
    "\n",
    "    return (\n",
    "        int(item_idx),\n",
    "        int(quantity),\n",
    "        price_sig,\n",
    "        int(context),\n",
    "        int(time_left),\n",
    "        age_sig,\n",
    "        tuple(sorted(bonus_lists)),\n",
    "        tuple(sorted(modifier_types)),\n",
    "    )\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Validation + noise-floor loop (updated for new dataloader/model)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_with_noise(model, val_loader, feature_stats, max_val_batches=10000, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Args\n",
    "    ----\n",
    "    model : AuctionTransformer (quantile outputs)\n",
    "    val_loader : DataLoader yielding dict-batches from AuctionDataset\n",
    "    feature_stats : {'means': tensor(5,), 'stds': tensor(5,), ...}\n",
    "    device : str\n",
    "    require_full_duration : bool\n",
    "        If True, restrict metrics/noise-floor to time_left==48.0 (like before).\n",
    "    \"\"\"\n",
    "\n",
    "    means = feature_stats[\"means\"].detach().cpu().numpy()   # first 5 auction cols\n",
    "    stds  = feature_stats[\"stds\"].detach().cpu().numpy()\n",
    "\n",
    "    total_mse   = 0.0\n",
    "    total_mae   = 0.0\n",
    "    total_items = 0.0\n",
    "\n",
    "    targets_by_sig = defaultdict(list)\n",
    "\n",
    "    # median channel = tau=0.5\n",
    "    # Try to find it; fallback to middle channel if needed.\n",
    "    if hasattr(model, \"quantiles\") and 0.5 in model.quantiles:\n",
    "        median_idx = model.quantiles.index(0.5)\n",
    "    else:\n",
    "        # Fallback: assume 3 quantiles [0.1,0.5,0.9]\n",
    "        median_idx = 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            if i > max_val_batches:\n",
    "                break\n",
    "            i += 1\n",
    "            # ---- move to device ------------------------------------------------\n",
    "            auctions        = batch['auctions'].to(device)           # (B,S,5) z-scored; bid/buyout were log1p pre-norm\n",
    "            item_index      = batch['item_index'].to(device)         # (B,S)\n",
    "            contexts        = batch['contexts'].to(device)           # (B,S)\n",
    "            bonus_lists     = batch['bonus_lists'].to(device)        # (B,S,K)\n",
    "            modifier_types  = batch['modifier_types'].to(device)     # (B,S,M)\n",
    "            modifier_values = batch['modifier_values'].to(device)    # (B,S,M) already log1p and normed\n",
    "            hour_of_week    = batch['hour_of_week'].to(device)       # (B,S)\n",
    "            time_offset     = batch['time_offset'].to(device)        # (B,S)\n",
    "            y               = batch['target'].to(device)             # (B,S) in HOURS (0..48)\n",
    "\n",
    "            current_hours_raw = batch['current_hours_raw'].to(device)  # (B,S)\n",
    "            time_left_raw     = batch['time_left_raw'].to(device)      # (B,S)\n",
    "\n",
    "            # ---- forward -------------------------------------------------------\n",
    "            y_hat_all = model((\n",
    "                auctions, item_index, contexts, bonus_lists,\n",
    "                modifier_types, modifier_values, hour_of_week, time_offset\n",
    "            ))                                                      # (B,S,Q)\n",
    "\n",
    "            # Take median channel (hours)\n",
    "            y_hat = y_hat_all[..., median_idx]                      # (B,S)\n",
    "\n",
    "            # ---- masks ---------------------------------------------------------\n",
    "            # real items\n",
    "            mask = (item_index != 0)\n",
    "\n",
    "            # only current auctions (time_offset == 0)\n",
    "            mask = mask & (time_offset == 0)\n",
    "            mask = mask & (current_hours_raw == 0.0)\n",
    "            mask = mask & (time_left_raw == 48.0)\n",
    "\n",
    "            mask_f = mask.float()\n",
    "\n",
    "            # ---- losses/metrics (in HOURS) ------------------------------------\n",
    "            # MSE and MAE on masked entries\n",
    "            sq_err_sum = ((y_hat - y) ** 2 * mask_f).sum()\n",
    "            abs_err_sum = (torch.abs(y_hat - y) * mask_f).sum()\n",
    "            n_items = mask_f.sum().item()\n",
    "\n",
    "            total_mse   += sq_err_sum.item()\n",
    "            total_mae   += abs_err_sum.item()\n",
    "            total_items += n_items\n",
    "\n",
    "            # ---- collect targets for irreducible-noise estimate ----------------\n",
    "            # Work on CPU/numpy\n",
    "            y_cpu               = y.detach().cpu().numpy()\n",
    "            mask_cpu            = mask.detach().cpu().numpy()\n",
    "            item_idx_cpu        = item_index.detach().cpu().numpy()\n",
    "            auctions_cpu        = auctions.detach().cpu().numpy()       # z-scores\n",
    "            contexts_cpu        = contexts.detach().cpu().numpy()\n",
    "            time_left_cpu       = time_left_raw.detach().cpu().numpy()\n",
    "            current_hours_cpu   = current_hours_raw.detach().cpu().numpy()\n",
    "            bonus_lists_cpu     = bonus_lists.detach().cpu().numpy()\n",
    "            modifier_types_cpu  = modifier_types.detach().cpu().numpy()\n",
    "\n",
    "            B, S, _ = auctions_cpu.shape\n",
    "            for b in range(B):\n",
    "                for s in range(S):\n",
    "                    if not mask_cpu[b, s]:\n",
    "                        continue\n",
    "\n",
    "                    # ---- undo standardisation + log1p for BUYOUT ---------------\n",
    "                    buyout_norm = auctions_cpu[b, s, 1]\n",
    "                    log_buyout  = buyout_norm * stds[1] + means[1]\n",
    "                    buyout_gold = np.expm1(log_buyout)\n",
    "\n",
    "                    # quantity column is linear but standardised\n",
    "                    qty_norm = auctions_cpu[b, s, 2]\n",
    "                    quantity = int(round(qty_norm * stds[2] + means[2]))\n",
    "\n",
    "                    # build signature (strip zeros from K/M dims)\n",
    "                    sig = auction_signature(\n",
    "                        item_idx_cpu[b, s],\n",
    "                        quantity,\n",
    "                        buyout_gold,\n",
    "                        contexts_cpu[b, s],\n",
    "                        time_left_cpu[b, s],\n",
    "                        current_hours_cpu[b, s],\n",
    "                        bonus_lists_cpu[b, s][bonus_lists_cpu[b, s] != 0],\n",
    "                        modifier_types_cpu[b, s][modifier_types_cpu[b, s] != 0],\n",
    "                    )\n",
    "\n",
    "                    targets_by_sig[sig].append(float(y_cpu[b, s]))\n",
    "\n",
    "    # ── aggregate --------------------------------------------------------------\n",
    "    if total_items > 0:\n",
    "        avg_mse = total_mse / total_items\n",
    "        avg_mae = total_mae / total_items\n",
    "    else:\n",
    "        avg_mse = float('nan')\n",
    "        avg_mae = float('nan')\n",
    "\n",
    "    # Pairwise absolute diffs for signatures with ≥2 targets (Bayesian lower bound)\n",
    "    noise_sum   = 0.0\n",
    "    noise_count = 0\n",
    "    for t_list in targets_by_sig.values():\n",
    "        if len(t_list) < 2:\n",
    "            continue\n",
    "        for a, b in itertools.combinations(t_list, 2):\n",
    "            noise_sum   += abs(a - b)\n",
    "            noise_count += 1\n",
    "\n",
    "    irreducible_mae = noise_sum / noise_count if noise_count else 0.0\n",
    "\n",
    "    print(f\"Validation MSE          : {avg_mse:.4f} (hours^2)\")\n",
    "    print(f\"Validation MAE          : {avg_mae:.4f} hours\")\n",
    "    print(f\"Bayesian lower-bound MAE: {irreducible_mae:.4f} hours\")\n",
    "\n",
    "    return avg_mse, avg_mae, irreducible_mae\n",
    "\n",
    "\n",
    "# Example call (match your training device variable/name)\n",
    "evaluate_with_noise(model, val_dataloader, feature_stats, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.inference import predict_dataframe\n",
    "\n",
    "predict_dataframe(model, df_auctions[df_auctions['item_index'] == 13815], prediction_time, feature_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
