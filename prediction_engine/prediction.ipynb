{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "BVwlGTGCcJRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsqGICHeE47W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/Projects/auction-classic"
      ],
      "metadata": {
        "id": "Nx9AAUuTFeoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjcQtWtoE47Y"
      },
      "outputs": [],
      "source": [
        "pairs = pd.read_csv('auction_indices.csv')\n",
        "pairs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare and balance data"
      ],
      "metadata": {
        "id": "i_XAyq9XvHHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pairs.describe()"
      ],
      "metadata": {
        "id": "BeSUcQqnvrUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = pairs[pairs['group_max'] < 50]\n",
        "pairs.describe()"
      ],
      "metadata": {
        "id": "knMeDyMD6Sys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVaV2INNE47Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_pairs, val_pairs = train_test_split(pairs, test_size=0.1, random_state=42, shuffle=False)\n",
        "\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Val pairs: {len(val_pairs)}\")\n",
        "\n",
        "val_pairs.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs_wotlk = train_pairs[train_pairs['expansion'] == 'wotlk']\n",
        "\n",
        "rows_to_delete = train_pairs_wotlk.sample(n=int(len(train_pairs_wotlk) * 0.85)).index\n",
        "train_pairs = train_pairs.drop(rows_to_delete)"
      ],
      "metadata": {
        "id": "XkvWyv8WMlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_pairs.expansion.value_counts())\n",
        "\n",
        "train_pairs.expansion.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "Da9pk2GoMB4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train_pairs['group_mean'], bins=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oRrffN74vIhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uniform_sample(df, column, n_samples):\n",
        "    bins = pd.cut(df[column], bins=48)\n",
        "\n",
        "    grouped = df.groupby(bins)\n",
        "\n",
        "    samples_per_bin = n_samples // len(grouped)\n",
        "    remainder = n_samples % len(grouped)\n",
        "\n",
        "    sampled_df = pd.DataFrame()\n",
        "    for _, group in grouped:\n",
        "        if len(group) > samples_per_bin:\n",
        "            sample = group.sample(n=samples_per_bin, replace=False)\n",
        "        else:\n",
        "            sample = group\n",
        "        sampled_df = pd.concat([sampled_df, sample])\n",
        "\n",
        "    if remainder > 0:\n",
        "        additional_sample = df.sample(n=remainder, replace=False)\n",
        "        sampled_df = pd.concat([sampled_df, additional_sample])\n",
        "\n",
        "    return sampled_df.sample(frac=1).reset_index(drop=True)  # Shuffle the final result\n",
        "\n",
        "train_pairs = uniform_sample(train_pairs, 'group_mean', n_samples=int(len(train_pairs)))\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "train_pairs.group_mean.hist(bins=10)"
      ],
      "metadata": {
        "id": "u4ATgIgx7J6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs.sample(5)"
      ],
      "metadata": {
        "id": "tIaj5IJN8DCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1xuJExpE47Y"
      },
      "outputs": [],
      "source": [
        "items = pd.read_csv('items.csv')\n",
        "n_items = len(items)\n",
        "\n",
        "item_to_index = {item_id: i + 2 for i, item_id in enumerate(items['item_id'])}\n",
        "item_to_index[0] = 0 # padding\n",
        "item_to_index[1] = 1 # unknown\n",
        "n_items"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "beqWnPlknkCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMygvurVE47Z"
      },
      "outputs": [],
      "source": [
        "class AuctionDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, pairs, item_to_index, path='sequences'):\n",
        "        self.pairs = pairs\n",
        "        self.column_map = {\n",
        "            'bid': 0,\n",
        "            'buyout': 1,\n",
        "            'quantity': 2,\n",
        "            'item_id': 3,\n",
        "            'time_left': 4,\n",
        "            'hours_since_first_appearance': 5\n",
        "        }\n",
        "        self.item_to_index = item_to_index\n",
        "        self.path = path\n",
        "\n",
        "        print(f\"Dataset size: {len(self)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs.iloc[idx]\n",
        "\n",
        "        record = pair['record']\n",
        "        item_id = pair['item_id']\n",
        "\n",
        "        date_time_obj = datetime.strptime(record, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        date_folder_name = date_time_obj.strftime(\"%Y-%m-%d\")\n",
        "        hour_folder_name = date_time_obj.strftime(\"%H\")\n",
        "\n",
        "        datetime_str = date_time_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        datetime_str = datetime_str.split(' ')[0] + ' 00:00:00'\n",
        "\n",
        "        data = torch.load(f'{self.path}/{date_folder_name}/{hour_folder_name}.pt')\n",
        "\n",
        "        X = torch.tensor(data[item_id])\n",
        "\n",
        "        y = X[:, -1]\n",
        "        X = X[:, :-1]\n",
        "\n",
        "        X[:, self.column_map['item_id']] = torch.tensor([self.item_to_index.get(int(item), 1) for item in X[:, self.column_map['item_id']]], dtype=torch.long)\n",
        "        X[:, self.column_map['time_left']] = X[:, self.column_map['time_left']] / 48.0\n",
        "        X[:, self.column_map['hours_since_first_appearance']] = X[:, self.column_map['hours_since_first_appearance']] / 48.0\n",
        "\n",
        "        X[:, self.column_map['bid']] = torch.log1p(X[:, self.column_map['bid']]) / 15.0\n",
        "        X[:, self.column_map['buyout']] = torch.log1p(X[:, self.column_map['buyout']]) / 15.0\n",
        "\n",
        "        X[:, self.column_map['quantity']] = X[:, self.column_map['quantity']] / 200.0\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vkmHhrEE47Z"
      },
      "outputs": [],
      "source": [
        "def collate_auctions(batch):\n",
        "    X, y = zip(*batch)\n",
        "\n",
        "    lengths = [x.size(0) for x in X]\n",
        "    lengths = torch.tensor(lengths)\n",
        "\n",
        "    max_length = lengths.max()\n",
        "\n",
        "    X = [F.pad(x, (0, 0, 0, max_length - x.size(0))) for x in X]\n",
        "    y = [F.pad(x, (0, max_length - x.size(0))) for x in y]\n",
        "\n",
        "    X = torch.stack(X)\n",
        "    y = torch.stack(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "train_dataset = AuctionDataset(pairs, item_to_index)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_auctions)\n",
        "\n",
        "iter_loader = iter(train_dataloader)\n",
        "X, y = next(iter_loader)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "FcSVYngOnnZA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGvjYoOsE47Z"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=5, item_index=3, embedding_size=16, hidden_size=16, dropout_p=0.1, bidirectional=True):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.item_index = item_index\n",
        "        n_items = len(item_to_index)\n",
        "\n",
        "        self.embedding = nn.Embedding(n_items, embedding_size)\n",
        "        self.rnn = nn.LSTM(input_size + embedding_size, hidden_size, batch_first=True, num_layers=2, bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, X):\n",
        "        item_ids = X[:, :, self.item_index].long()\n",
        "\n",
        "        X = torch.cat([X[:, :, :self.item_index], X[:, :, self.item_index + 1:]], dim=2)\n",
        "\n",
        "        item_embeddings = self.dropout(self.embedding(item_ids))\n",
        "\n",
        "        X = torch.cat([X, item_embeddings], dim=2)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(X)\n",
        "\n",
        "        return output, (hidden, cell)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bidirectional=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        output_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, bidirectional=bidirectional)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(output_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden):\n",
        "        output, _ = self.rnn(encoder_outputs, encoder_hidden)\n",
        "        output = self.projection(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class AuctionPredictor(nn.Module):\n",
        "    def __init__(self, input_size=5, encoder_hidden_size=16, decoder_hidden_size=16, item_index=3, embedding_size=16, dropout_p=0.1, bidirectional=True):\n",
        "        super(AuctionPredictor, self).__init__()\n",
        "        decoder_input_size = encoder_hidden_size * 2 if bidirectional else encoder_hidden_size\n",
        "        self.encoder = Encoder(input_size, item_index, embedding_size, encoder_hidden_size, dropout_p, bidirectional=bidirectional)\n",
        "        self.decoder = Decoder(decoder_input_size, decoder_hidden_size, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, X):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(X)\n",
        "        decoder_outputs = self.decoder(encoder_outputs, encoder_hidden)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AuctionDataset(train_pairs, item_to_index)\n",
        "val_dataset = AuctionDataset(val_pairs, item_to_index)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_auctions, num_workers=8, prefetch_factor=4)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=collate_auctions, num_workers=8, prefetch_factor=4)"
      ],
      "metadata": {
        "id": "-ndNrhFcUmFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGoHJnBEE47a"
      },
      "outputs": [],
      "source": [
        "embedding_size = 128\n",
        "encoder_hidden_size = 256\n",
        "decoder_hidden_size = 256\n",
        "epochs = 1\n",
        "save_every_iters = 5000\n",
        "\n",
        "model = AuctionPredictor(input_size=5,\n",
        "                         encoder_hidden_size=encoder_hidden_size,\n",
        "                         decoder_hidden_size=decoder_hidden_size,\n",
        "                         item_index=3,\n",
        "                         embedding_size=embedding_size,\n",
        "                         dropout_p=0.2,\n",
        "                         bidirectional=False\n",
        "                         ).to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "load_checkpoint = False\n",
        "\n",
        "if load_checkpoint:\n",
        "  checkpoint = torch.load('checkpoints/checkpoint_epoch_4.pt')\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=1e-06, total_iters=total_steps)\n",
        "criterion = nn.MSELoss(reduction='sum')\n",
        "\n",
        "print(f'Iterations per epoch: {len(train_dataloader)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"auction-classic\", config={\n",
        "    \"epochs\": epochs,\n",
        "    \"batch_size\": train_loader.batch_size,\n",
        "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "    \"encoder_hidden_size\": encoder_hidden_size,\n",
        "    \"decoder_hidden_size\": decoder_hidden_size,\n",
        "    \"model_size\": sum(p.numel() for p in model.parameters()),\n",
        "    \"embedding_size\": embedding_size,\n",
        "    \"bidirectional\": False,\n",
        "    \"dropout\": 0.2\n",
        "})"
      ],
      "metadata": {
        "id": "6-OAt08dcXyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "GyMw3icYnqXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuGhUuNIE47a"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, iters, checkpoint_path='checkpoints'):\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    checkpoint_file = os.path.join(checkpoint_path, f\"checkpoint_epoch_{epoch}_iter_{iters}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'iter': iters,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, checkpoint_file)\n",
        "    print(f\"Checkpoint saved at {checkpoint_file}\")\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    eval_steps,\n",
        "    device,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    lr_scheduler\n",
        "):\n",
        "    print(\"Starting training for\", epochs, \"epochs\")\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "\n",
        "        mse_losses = []\n",
        "        mae_losses = []\n",
        "\n",
        "        for i, (X, y) in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred = model(X)\n",
        "\n",
        "            loss = criterion(y_pred, y.unsqueeze(2))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                mae = F.l1_loss(y_pred, y.unsqueeze(2), reduction='sum')\n",
        "                n = (y != 0).sum().item()\n",
        "                mae /= n\n",
        "\n",
        "            mse_losses.append(loss.item() / n)\n",
        "            mae_losses.append(mae.item())\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                mse_loss_avg = np.mean(mse_losses)\n",
        "                mae_loss_avg = np.mean(mae_losses)\n",
        "                lr = lr_scheduler.get_last_lr()[0]\n",
        "                print(f\"Epoch {epoch} Iteration {i} Loss {mse_loss_avg} MAE {mae_loss_avg} LR {lr}\")\n",
        "\n",
        "                wandb.log({\n",
        "                  \"train/mse_loss\": mse_loss_avg,\n",
        "                  \"train/mae_loss\": mae_loss_avg,\n",
        "                  \"train/learning_rate\": lr,\n",
        "                  \"epoch\": epoch\n",
        "                })\n",
        "\n",
        "                mse_losses = []\n",
        "                mae_losses = []\n",
        "\n",
        "            if (i + 1) % eval_steps == 0:\n",
        "              val_loss, val_mae = evaluate(model, val_loader, device, criterion)\n",
        "              wandb.log({\n",
        "                \"val/mse_loss\": val_loss,\n",
        "                \"val/mae_loss\": val_mae,\n",
        "                \"epoch\": epoch\n",
        "              })\n",
        "\n",
        "            if (i + 1) % save_every_iters == 0:\n",
        "              save_checkpoint(model, optimizer, epoch, i)\n",
        "\n",
        "        save_checkpoint(model, optimizer, epoch, len(train_loader))\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model,\n",
        "    val_loader,\n",
        "    device,\n",
        "    criterion\n",
        "):\n",
        "    print(\"Evaluating model\")\n",
        "    model.eval()\n",
        "\n",
        "    mse_losses = []\n",
        "    mae_losses = []\n",
        "\n",
        "    for i, (X, y) in enumerate(val_loader):\n",
        "      if i >= 100:\n",
        "        break\n",
        "\n",
        "      if i % 15 == 0:\n",
        "        print(f\"Evaluating step {i}\")\n",
        "\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      y_pred = model(X)\n",
        "\n",
        "      loss = criterion(y_pred, y.unsqueeze(2))\n",
        "\n",
        "      mae = F.l1_loss(y_pred, y.unsqueeze(2), reduction='sum')\n",
        "      n = (y != 0).sum().item()\n",
        "      mae /= n\n",
        "\n",
        "      mse_losses.append(loss.item() / n)\n",
        "      mae_losses.append(mae.item())\n",
        "\n",
        "      if i % 25 == 0:\n",
        "        print(y[0][:10])\n",
        "        print(y_pred[0,:, 0][:10])\n",
        "\n",
        "    mse_loss_avg = np.mean(mse_losses)\n",
        "    mae_loss_avg = np.mean(mae_losses)\n",
        "\n",
        "    print(f\"Validation loss: {mse_loss_avg} MAE: {mae_loss_avg}\")\n",
        "    model.train()\n",
        "\n",
        "    return mse_loss_avg, mae_loss_avg\n",
        "\n",
        "train(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    epochs,\n",
        "    eval_steps=250,\n",
        "    device=device,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    lr_scheduler=lr_scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "TLGEHyYW0b5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "jRp59SWqnsmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)"
      ],
      "metadata": {
        "id": "ZvjBFA7eghr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:, val_dataset.column_map['bid']] = X[:, val_dataset.column_map['bid']] / 1000\n",
        "X[:, val_dataset.column_map['buyout']] = X[:, val_dataset.column_map['buyout']] / 1000\n",
        "\n",
        "print(X)\n",
        "\n",
        "pred = model(X.unsqueeze(0).to('cpu'))\n",
        "\n",
        "X[:, val_dataset.column_map['bid']] = X[:, val_dataset.column_map['bid']] * 1000\n",
        "X[:, val_dataset.column_map['buyout']] = X[:, val_dataset.column_map['buyout']] * 1000\n",
        "X[:, val_dataset.column_map['quantity']] = X[:, val_dataset.column_map['quantity']] * 200\n",
        "X[:, val_dataset.column_map['hours_since_first_appearance']] = X[:, val_dataset.column_map['hours_since_first_appearance']] * 48\n",
        "X[:, val_dataset.column_map['time_left']] = X[:, val_dataset.column_map['time_left']] * 48\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "yaj_kmFnlIpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_item = {i + 2: item_id for i, item_id in enumerate(items['item_id'])}\n",
        "index_to_item[0] = 0\n",
        "index_to_item[1] = 1\n",
        "\n",
        "print(index_to_item.get(7631))"
      ],
      "metadata": {
        "id": "fEZqZr_uNICQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}