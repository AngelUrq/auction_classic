{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3364,
     "status": "ok",
     "timestamp": 1729270089178,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "BVwlGTGCcJRb",
    "outputId": "7b71789e-e97c-463f-ec99-87523000df9e"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_dir = \"../\"\n",
    "zip_file_path = \"generated.zip\"\n",
    "\n",
    "if not os.path.exists(target_dir + 'generated'):\n",
    "    print(f\"The directory {target_dir} does not exist. Proceeding with download.\")\n",
    "\n",
    "    !apt-get update\n",
    "    !apt-get install unzip\n",
    "    \n",
    "    !curl \"https://drive.usercontent.google.com/download?id=1_z-BLK2nlweOgRiDJXhPwsDR5qjqe9Nm&confirm=xxx\" -o {zip_file_path}\n",
    "    !mkdir -p {target_dir}\n",
    "    \n",
    "    !unzip {zip_file_path} -d {target_dir}\n",
    "    \n",
    "    print(f\"File downloaded and extracted to {target_dir}\")\n",
    "    \n",
    "    !rm {zip_file_path}\n",
    "else:\n",
    "    print(f\"The directory {target_dir} already exists. No action taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8394,
     "status": "ok",
     "timestamp": 1729270097569,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "gsqGICHeE47W",
    "outputId": "5fbdf5a0-d09a-40e1-a47a-2a80b8007caa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 16985,
     "status": "ok",
     "timestamp": 1729270137493,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "KjcQtWtoE47Y",
    "outputId": "12bf3488-0c75-42ff-b77c-f5dc88a524e0"
   },
   "outputs": [],
   "source": [
    "pairs = pd.read_csv('../generated/auction_indices.csv')\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_XAyq9XvHHu"
   },
   "source": [
    "## Prepare and balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 2282,
     "status": "ok",
     "timestamp": 1729270139773,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "BeSUcQqnvrUH",
    "outputId": "324777e2-c906-482f-9349-d56e1945373f"
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pairs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[['group_len']].quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 2725,
     "status": "ok",
     "timestamp": 1729270142496,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "knMeDyMD6Sys",
    "outputId": "25d8fe21-716d-4bfe-a81b-977e6046fb57"
   },
   "outputs": [],
   "source": [
    "pairs = pairs[pairs['group_max'] < 50]\n",
    "pairs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1729270143003,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "CVaV2INNE47Y",
    "outputId": "53d471b4-79a7-4a01-8f8b-423de5afdb84"
   },
   "outputs": [],
   "source": [
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.1, random_state=42, shuffle=False)\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "print(f\"Val pairs: {len(val_pairs)}\")\n",
    "\n",
    "val_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5582,
     "status": "ok",
     "timestamp": 1729270148990,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "XkvWyv8WMlg-"
   },
   "outputs": [],
   "source": [
    "train_pairs_wotlk = train_pairs[train_pairs['expansion'] == 'wotlk']\n",
    "\n",
    "rows_to_delete = train_pairs_wotlk.sample(n=int(len(train_pairs_wotlk) * 0.85)).index\n",
    "train_pairs = train_pairs.drop(rows_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1729270149598,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "Da9pk2GoMB4w",
    "outputId": "4bad63f9-db6e-4da9-8db8-3b979e8ebd27"
   },
   "outputs": [],
   "source": [
    "print(train_pairs.expansion.value_counts())\n",
    "\n",
    "train_pairs.expansion.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729270149598,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "oRrffN74vIhi",
    "outputId": "cd8b1691-8251-4840-f6ea-3c14f979c1d5"
   },
   "outputs": [],
   "source": [
    "plt.hist(train_pairs['group_mean'], bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "executionInfo": {
     "elapsed": 1785,
     "status": "ok",
     "timestamp": 1729270151378,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "u4ATgIgx7J6n",
    "outputId": "f4827bcf-64ce-4783-c517-8a9a29bb80b2"
   },
   "outputs": [],
   "source": [
    "def uniform_sample(df, column, n_samples):\n",
    "    bins = pd.cut(df[column], bins=48)\n",
    "\n",
    "    grouped = df.groupby(bins)\n",
    "\n",
    "    samples_per_bin = n_samples // len(grouped)\n",
    "    remainder = n_samples % len(grouped)\n",
    "\n",
    "    sampled_df = pd.DataFrame()\n",
    "    for _, group in grouped:\n",
    "        if len(group) > samples_per_bin:\n",
    "            sample = group.sample(n=samples_per_bin, replace=False)\n",
    "        else:\n",
    "            sample = group\n",
    "        sampled_df = pd.concat([sampled_df, sample])\n",
    "\n",
    "    if remainder > 0:\n",
    "        additional_sample = df.sample(n=remainder, replace=False)\n",
    "        sampled_df = pd.concat([sampled_df, additional_sample])\n",
    "\n",
    "    return sampled_df.sample(frac=1).reset_index(drop=True)  # Shuffle the final result\n",
    "\n",
    "train_pairs = uniform_sample(train_pairs, 'group_mean', n_samples=int(len(train_pairs)))\n",
    "print(f\"Train pairs: {len(train_pairs)}\")\n",
    "train_pairs.group_mean.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1729270151906,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "S1xuJExpE47Y",
    "outputId": "c5ea6f75-fe52-4f9e-d610-9919458fd7e1"
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv('../data/items.csv')\n",
    "n_items = len(items)\n",
    "\n",
    "item_to_index = {item_id: i + 2 for i, item_id in enumerate(items['item_id'])}\n",
    "item_to_index[0] = 0 # padding\n",
    "item_to_index[1] = 1 # unknown\n",
    "n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beqWnPlknkCw"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729270151906,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "dMygvurVE47Z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "class AuctionDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, pairs, item_to_index, path='sequences'):\n",
    "        self.pairs = pairs\n",
    "        self.column_map = {\n",
    "            'bid': 0,\n",
    "            'buyout': 1,\n",
    "            'quantity': 2,\n",
    "            'item_id': 3,\n",
    "            'time_left': 4,\n",
    "            'hours_since_first_appearance': 5\n",
    "        }\n",
    "        self.item_to_index = item_to_index\n",
    "        self.path = path\n",
    "\n",
    "        print(f\"Dataset size: {len(self)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs.iloc[idx]\n",
    "\n",
    "        record = pair['record']\n",
    "        item_id = pair['item_id']\n",
    "\n",
    "        # Parse the record timestamp\n",
    "        date_time_obj = datetime.strptime(record, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_folder_name = date_time_obj.strftime(\"%Y-%m-%d\")\n",
    "        hour_folder_name = date_time_obj.strftime(\"%H\")\n",
    "\n",
    "        file_path = f'{self.path}/{date_folder_name}/{hour_folder_name}.pt'\n",
    "\n",
    "        data = torch.load(file_path)\n",
    "        X = torch.tensor(data[item_id])\n",
    "\n",
    "        y = X[:, -1] - X[:, self.column_map['hours_since_first_appearance']]\n",
    "        X = X[:, :-1]\n",
    "\n",
    "        X[:, self.column_map['item_id']] = torch.tensor([self.item_to_index.get(int(item), 1) for item in X[:, self.column_map['item_id']]], dtype=torch.long)\n",
    "        X[:, self.column_map['time_left']] = X[:, self.column_map['time_left']] / 48.0\n",
    "        X[:, self.column_map['hours_since_first_appearance']] = X[:, self.column_map['hours_since_first_appearance']] / 48.0\n",
    "        X[:, self.column_map['bid']] = torch.log1p(X[:, self.column_map['bid']]) / 15.0\n",
    "        X[:, self.column_map['buyout']] = torch.log1p(X[:, self.column_map['buyout']]) / 15.0\n",
    "        X[:, self.column_map['quantity']] = X[:, self.column_map['quantity']] / 200.0\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729270151907,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "1vkmHhrEE47Z"
   },
   "outputs": [],
   "source": [
    "def collate_auctions(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    X, y = zip(*batch)\n",
    "\n",
    "    lengths = torch.LongTensor([x.size(0) for x in X])\n",
    "\n",
    "    max_length = lengths.max()\n",
    "\n",
    "    X = [F.pad(x, (0, 0, 0, max_length - x.size(0))) for x in X]\n",
    "    y = [F.pad(x, (0, max_length - x.size(0))) for x in y]\n",
    "\n",
    "    X = torch.stack(X)\n",
    "    y = torch.stack(y)\n",
    "\n",
    "    return X, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcSVYngOnnZA"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729270151907,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "OGvjYoOsE47Z"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=5, item_index=3, embedding_size=16, hidden_size=16, dropout_p=0.1, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.item_index = item_index\n",
    "        n_items = len(item_to_index)\n",
    "\n",
    "        self.embedding = nn.Embedding(n_items, embedding_size)\n",
    "        self.rnn = nn.GRU(input_size + embedding_size, hidden_size, batch_first=True, num_layers=2, bidirectional=bidirectional, dropout=dropout_p)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        item_ids = X[:, :, self.item_index].long()\n",
    "        X = torch.cat([X[:, :, :self.item_index], X[:, :, self.item_index + 1:]], dim=2)\n",
    "        item_embeddings = self.dropout(self.embedding(item_ids))\n",
    "\n",
    "        X = torch.cat([X, item_embeddings], dim=2)\n",
    "        X_packed = pack_padded_sequence(X, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        output_packed, hidden = self.rnn(X_packed)\n",
    "\n",
    "        return output_packed, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bidirectional=True, dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True, num_layers=2, bidirectional=bidirectional, dropout=dropout_p)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden):\n",
    "        output_packed, _ = self.rnn(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        output, _ = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        output = self.projection(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class AuctionPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, encoder_hidden_size=16, decoder_hidden_size=16, item_index=3, embedding_size=16, dropout_p=0.1, bidirectional=True):\n",
    "        super(AuctionPredictor, self).__init__()\n",
    "        decoder_input_size = encoder_hidden_size * 2 if bidirectional else encoder_hidden_size\n",
    "        self.encoder = Encoder(input_size, item_index, embedding_size, encoder_hidden_size, dropout_p, bidirectional=bidirectional)\n",
    "        self.decoder = Decoder(decoder_input_size, decoder_hidden_size, bidirectional=bidirectional, dropout_p=dropout_p)\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(X, lengths)\n",
    "        decoder_outputs = self.decoder(encoder_outputs, encoder_hidden)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729270151907,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "-ndNrhFcUmFL",
    "outputId": "6962e361-1b92-4da0-b0f0-58a39cafe50f"
   },
   "outputs": [],
   "source": [
    "train_dataset = AuctionDataset(train_pairs, item_to_index, path='../generated/sequences')\n",
    "val_dataset = AuctionDataset(val_pairs, item_to_index, path='../generated/sequences')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_auctions, num_workers=8, prefetch_factor=8)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=True, collate_fn=collate_auctions, num_workers=8, prefetch_factor=8)\n",
    "\n",
    "# Test dataloader\n",
    "iter_loader = iter(train_dataloader)\n",
    "X, y, lengths = next(iter_loader)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4023,
     "status": "ok",
     "timestamp": 1729270155928,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "HGoHJnBEE47a",
    "outputId": "bea7138a-047a-4cb1-8d72-ece5efed058d"
   },
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "encoder_hidden_size = 1024\n",
    "decoder_hidden_size = 1024\n",
    "epochs = 1\n",
    "save_every_iters = 2500\n",
    "dropout = 0.1\n",
    "bidirectional = False\n",
    "\n",
    "model = AuctionPredictor(input_size=5,\n",
    "                         encoder_hidden_size=encoder_hidden_size,\n",
    "                         decoder_hidden_size=decoder_hidden_size,\n",
    "                         item_index=3,\n",
    "                         embedding_size=embedding_size,\n",
    "                         dropout_p=dropout,\n",
    "                         bidirectional=bidirectional\n",
    "                         ).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "load_checkpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "  checkpoint = torch.load('checkpoints/checkpoint_epoch_4.pt')\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, verbose=True)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "print(f'Iterations per epoch: {len(train_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 9540,
     "status": "ok",
     "timestamp": 1729270165464,
     "user": {
      "displayName": "Ángel Zenteno",
      "userId": "00182779788459853828"
     },
     "user_tz": -120
    },
    "id": "6-OAt08dcXyH",
    "outputId": "feb85749-4264-4823-b3b6-651a2033f26b"
   },
   "outputs": [],
   "source": [
    "enable_logging = True\n",
    "\n",
    "if enable_logging:\n",
    "  print(\"Logging enabled\")\n",
    "  wandb.init(project=\"auction-classic\", config={\n",
    "      \"epochs\": epochs,\n",
    "      \"batch_size\": train_dataloader.batch_size,\n",
    "      \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "      \"encoder_hidden_size\": encoder_hidden_size,\n",
    "      \"decoder_hidden_size\": decoder_hidden_size,\n",
    "      \"model_size\": sum(p.numel() for p in model.parameters()),\n",
    "      \"embedding_size\": embedding_size,\n",
    "      \"bidirectional\": bidirectional,\n",
    "      \"dropout\": dropout\n",
    "  })\n",
    "else:\n",
    "  print(\"Logging disabled\")\n",
    "  wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyMw3icYnqXo"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuGhUuNIE47a",
    "outputId": "d7db1411-1ad4-4954-ba1a-094c09c0ce0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, iters, checkpoint_path='checkpoints'):\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(checkpoint_path, f\"checkpoint_epoch_{epoch}_iter_{iters}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'iter': iters,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_file)\n",
    "    print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    eval_steps,\n",
    "    device,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    lr_scheduler\n",
    "):\n",
    "    print(\"Starting training for\", epochs, \"epochs\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "\n",
    "        mse_losses = []\n",
    "        mae_losses = []\n",
    "\n",
    "        for i, (X, y, lengths) in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(X, lengths)\n",
    "            mask = (y != 0).float().unsqueeze(2)\n",
    "\n",
    "            loss = criterion(y_pred * mask, y.unsqueeze(2)) / mask.sum()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mae = F.l1_loss(y_pred * mask, y.unsqueeze(2) * mask, reduction='sum') / mask.sum()\n",
    "\n",
    "            mse_losses.append(loss.item())\n",
    "            mae_losses.append(mae.item())\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                mse_loss_avg = np.mean(mse_losses)\n",
    "                mae_loss_avg = np.mean(mae_losses)\n",
    "                lr = optimizer.param_groups[0]['lr']#lr_scheduler.get_last_lr()[0]\n",
    "                print(f\"Epoch {epoch} Iteration {i} Loss {mse_loss_avg} MAE {mae_loss_avg} LR {lr}\")\n",
    "                lr_scheduler.step(mse_loss_avg)\n",
    "\n",
    "                wandb.log({\n",
    "                  \"train/mse_loss\": mse_loss_avg,\n",
    "                  \"train/mae_loss\": mae_loss_avg,\n",
    "                  \"train/learning_rate\": lr,\n",
    "                  \"epoch\": epoch,\n",
    "                  \"iter\": i\n",
    "                })\n",
    "\n",
    "                mse_losses = []\n",
    "                mae_losses = []\n",
    "\n",
    "            if (i + 1) % eval_steps == 0:\n",
    "              val_loss, val_mae = evaluate(model, val_loader, device, criterion)\n",
    "              wandb.log({\n",
    "                \"val/mse_loss\": val_loss,\n",
    "                \"val/mae_loss\": val_mae,\n",
    "                \"epoch\": epoch\n",
    "              })\n",
    "\n",
    "            if (i + 1) % save_every_iters == 0:\n",
    "              save_checkpoint(model, optimizer, epoch, i)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, len(train_loader))\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device,\n",
    "    criterion\n",
    "):\n",
    "    print(\"Evaluating model\")\n",
    "    model.eval()\n",
    "\n",
    "    mse_losses = []\n",
    "    mae_losses = []\n",
    "\n",
    "    for i, (X, y, lengths) in enumerate(val_loader):\n",
    "\n",
    "      if i >= 300:\n",
    "        break\n",
    "\n",
    "      if i % 15 == 0:\n",
    "        print(f\"Evaluating step {i}\")\n",
    "\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      y_pred = model(X, lengths)\n",
    "\n",
    "      mask = (y != 0).float().unsqueeze(2)\n",
    "      loss = criterion(y_pred * mask, y.unsqueeze(2)) / mask.sum()\n",
    "      mae = F.l1_loss(y_pred * mask, y.unsqueeze(2) * mask, reduction='sum') / mask.sum()\n",
    "\n",
    "      mse_losses.append(loss.item())\n",
    "      mae_losses.append(mae.item())\n",
    "\n",
    "      if i % 25 == 0:\n",
    "        print(f\"Evaluating step {i}\")\n",
    "        print(y[0][:10])\n",
    "        print(y_pred[0,:, 0][:10])\n",
    "\n",
    "    mse_loss_avg = np.mean(mse_losses)\n",
    "    mae_loss_avg = np.mean(mae_losses)\n",
    "\n",
    "    print(f\"Validation loss: {mse_loss_avg} MAE: {mae_loss_avg}\")\n",
    "    model.train()\n",
    "\n",
    "    return mse_loss_avg, mae_loss_avg\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs,\n",
    "    eval_steps=250,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    lr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLGEHyYW0b5l"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRp59SWqnsmR"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvjBFA7eghr9"
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaj_kmFnlIpq"
   },
   "outputs": [],
   "source": [
    "X[:, val_dataset.column_map['bid']] = X[:, val_dataset.column_map['bid']] / 1000\n",
    "X[:, val_dataset.column_map['buyout']] = X[:, val_dataset.column_map['buyout']] / 1000\n",
    "\n",
    "print(X)\n",
    "\n",
    "pred = model(X.unsqueeze(0).to('cpu'))\n",
    "\n",
    "X[:, val_dataset.column_map['bid']] = X[:, val_dataset.column_map['bid']] * 1000\n",
    "X[:, val_dataset.column_map['buyout']] = X[:, val_dataset.column_map['buyout']] * 1000\n",
    "X[:, val_dataset.column_map['quantity']] = X[:, val_dataset.column_map['quantity']] * 200\n",
    "X[:, val_dataset.column_map['hours_since_first_appearance']] = X[:, val_dataset.column_map['hours_since_first_appearance']] * 48\n",
    "X[:, val_dataset.column_map['time_left']] = X[:, val_dataset.column_map['time_left']] * 48\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEZqZr_uNICQ"
   },
   "outputs": [],
   "source": [
    "index_to_item = {i + 2: item_id for i, item_id in enumerate(items['item_id'])}\n",
    "index_to_item[0] = 0\n",
    "index_to_item[1] = 1\n",
    "\n",
    "print(index_to_item.get(7631))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
