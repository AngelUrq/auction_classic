{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os\n",
    "from mysql.connector.pooling import MySQLConnectionPool, PooledMySQLConnection\n",
    "from tqdm import tqdm\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"password\",\n",
    "  database=\"auction_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"  \n",
    "    CREATE TABLE AuctionHours AS\n",
    "    SELECT \n",
    "        a.auction_id,\n",
    "        a.item_id,\n",
    "        a.bid / 10000.0 AS bid, \n",
    "        a.buyout / 10000.0 AS buyout, \n",
    "        a.quantity,\n",
    "        h.first_appearance_timestamp,\n",
    "        h.total_hours_on_sale\n",
    "    FROM \n",
    "        Auctions a\n",
    "    INNER JOIN (\n",
    "        SELECT \n",
    "            ae.auction_id, \n",
    "            COUNT(ae.record) AS total_hours_on_sale, \n",
    "            MIN(ae.record) AS first_appearance_timestamp\n",
    "        FROM \n",
    "            ActionEvents ae\n",
    "        GROUP BY \n",
    "            ae.auction_id\n",
    "    ) h \n",
    "    ON \n",
    "        a.auction_id = h.auction_id;\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE INDEX index_item\n",
    "    ON AuctionHours (item_id);\n",
    "\"\"\")\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"  \n",
    "    SELECT DISTINCT ae.record, a.item_id\n",
    "    FROM Auctions a\n",
    "    JOIN ActionEvents ae ON a.auction_id = ae.auction_id;       \n",
    "\"\"\")\n",
    "\n",
    "data = cursor.fetchall()\n",
    "\n",
    "headers = [column[0] for column in cursor.description]\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "pairs = pd.DataFrame(data, columns=headers)\n",
    "pairs.to_csv('auction_indices.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.read_csv('auction_indices.csv')\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "print(train_pairs.shape, val_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample():\n",
    "    pair = pairs.sample(1)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT ae.auction_id, \n",
    "        ae.record, \n",
    "        ae.time_left,\n",
    "        bid / 10000.0 as bid, \n",
    "        buyout / 10000.0 as buyout, \n",
    "        ah.quantity, \n",
    "        ah.item_id,\n",
    "        ah.first_appearance_timestamp,\n",
    "        CAST(TIMESTAMPDIFF(HOUR, ah.first_appearance_timestamp, ae.record) AS SIGNED) AS hours_since_first_appearance,\n",
    "        ah.total_hours_on_sale\n",
    "        FROM ActionEvents ae\n",
    "        INNER JOIN (\n",
    "            SELECT auction_id, item_id, bid, buyout, quantity, first_appearance_timestamp, total_hours_on_sale\n",
    "            FROM AuctionHours ah\n",
    "            WHERE ah.item_id = {pair['item_id'].values[0]}\n",
    "        ) ah ON ah.auction_id = ae.auction_id \n",
    "        WHERE ae.record = \"{pair['record'].values[0]}\";\n",
    "    \"\"\")\n",
    "\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    headers = [column[0] for column in cursor.description]\n",
    "\n",
    "    sample = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "    sample['first_appearance_timestamp'] = pd.to_datetime(sample['first_appearance_timestamp'])\n",
    "    sample['record'] = pd.to_datetime(sample['record'])\n",
    "\n",
    "    sample['hours_on_sale'] = sample['total_hours_on_sale'] - sample['hours_since_first_appearance']\n",
    "\n",
    "    cursor.close()\n",
    "        \n",
    "    return sample\n",
    "\n",
    "sample = get_random_sample()\n",
    "print(f\"Median buyout: {sample['buyout'].median()}\")\n",
    "print(f\"Median bid: {sample['bid'].median()}\")\n",
    "    \n",
    "sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('items.csv')\n",
    "n_items = len(items)\n",
    "\n",
    "item_to_index = {item_id: i + 1 for i, item_id in enumerate(items['item_id'])}\n",
    "item_to_index[0] = 0\n",
    "n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuctionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, conn, pairs, item_to_index):\n",
    "        self.conn = conn\n",
    "        self.pairs = pairs\n",
    "        self.time_left_to_int = {\n",
    "            'VERY_LONG': 48,\n",
    "            'LONG': 24,\n",
    "            'MEDIUM': 12,\n",
    "            'SHORT': 2\n",
    "        }\n",
    "        self.item_to_index = item_to_index\n",
    "        \n",
    "        print(f\"Dataset size: {len(self)}\")\n",
    "        \n",
    "        #self.pool = MySQLConnectionPool(\n",
    "        #    pool_name=\"mypool\",\n",
    "        #    pool_size=32,\n",
    "        #    host=\"localhost\",\n",
    "        #    user=\"root\",\n",
    "        #    password=\"password\",\n",
    "        #    database=\"auction_db\"\n",
    "        #)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs.iloc[idx]\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "    \n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT ae.auction_id, \n",
    "            ae.record, \n",
    "            ae.time_left,\n",
    "            bid / 10000.0 as bid, \n",
    "            buyout / 10000.0 as buyout, \n",
    "            ah.quantity, \n",
    "            ah.item_id,\n",
    "            ah.first_appearance_timestamp,\n",
    "            CAST(TIMESTAMPDIFF(HOUR, ah.first_appearance_timestamp, ae.record) AS SIGNED) AS hours_since_first_appearance,\n",
    "            ah.total_hours_on_sale\n",
    "            FROM ActionEvents ae\n",
    "            INNER JOIN (\n",
    "                SELECT auction_id, item_id, bid, buyout, quantity, first_appearance_timestamp, total_hours_on_sale\n",
    "                FROM AuctionHours ah\n",
    "                WHERE ah.item_id = {pair.values[1]}\n",
    "            ) ah ON ah.auction_id = ae.auction_id \n",
    "            WHERE ae.record = \"{pair.values[0]}\";\n",
    "        \"\"\")\n",
    "\n",
    "        data = cursor.fetchall()\n",
    "\n",
    "        headers = [column[0] for column in cursor.description]\n",
    "\n",
    "        sample = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "        sample['first_appearance_timestamp'] = pd.to_datetime(sample['first_appearance_timestamp'])\n",
    "        sample['record'] = pd.to_datetime(sample['record'])\n",
    "\n",
    "        sample['hours_on_sale'] = sample['total_hours_on_sale'] - sample['hours_since_first_appearance']\n",
    "\n",
    "        cursor.close()\n",
    "        \n",
    "        sample = sample.drop(columns=['auction_id', 'record', 'first_appearance_timestamp', 'total_hours_on_sale'])\n",
    "        \n",
    "        numerical_columns = [\n",
    "            'bid', \n",
    "            'buyout', \n",
    "            'quantity'\n",
    "        ]\n",
    "        \n",
    "        categorical_columns_ordinal = [\n",
    "            'item_id',\n",
    "            'time_left',\n",
    "            'hours_since_first_appearance' \n",
    "        ]\n",
    "\n",
    "        X = sample[numerical_columns + categorical_columns_ordinal]\n",
    "        y = sample['hours_on_sale']\n",
    "        \n",
    "        X['time_left'] = X['time_left'].map(self.time_left_to_int) / 48.0\n",
    "        X['item_id'] = X['item_id'].map(lambda x: self.item_to_index.get(x, 0))\n",
    "        X['hours_since_first_appearance'] = X['hours_since_first_appearance'] / 48.0\n",
    "        \n",
    "        num_transformer = RobustScaler()\n",
    "\n",
    "        column_transformer = make_column_transformer(\n",
    "            (num_transformer, numerical_columns),\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        X = column_transformer.fit_transform(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_auctions(batch):\n",
    "    X, y = zip(*batch)\n",
    "    \n",
    "    max_length = max([x.size(0) for x in X])\n",
    "    \n",
    "    X = [F.pad(x, (0, 0, 0, max_length - x.size(0))) for x in X]\n",
    "    y = [F.pad(x, (0, max_length - x.size(0))) for x in y]\n",
    "    \n",
    "    X = torch.stack(X)\n",
    "    y = torch.stack(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "train_dataset = AuctionDataset(conn, pairs, item_to_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_auctions)\n",
    "\n",
    "iter_loader = iter(train_loader)\n",
    "X, y = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=5, item_index=3, embedding_size=16, hidden_size=16, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.item_index = item_index\n",
    "        n_items = len(item_to_index)\n",
    "\n",
    "        self.embedding = nn.Embedding(n_items, embedding_size)\n",
    "        self.rnn = nn.LSTM(input_size + embedding_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        item_ids = X[:, :, self.item_index].long()\n",
    "        \n",
    "        X = torch.cat([X[:, :, :self.item_index], X[:, :, self.item_index + 1:]], dim=2)\n",
    "        \n",
    "        item_embeddings = self.dropout(self.embedding(item_ids))\n",
    "        \n",
    "        X = torch.cat([X, item_embeddings], dim=2)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(X)\n",
    "        \n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_hidden):\n",
    "        output, _ = self.rnn(encoder_outputs, encoder_hidden)\n",
    "        output = self.projection(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class AuctionPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, encoder_hidden_size=16, decoder_hidden_size=16, item_index=3, embedding_size=16, dropout_p=0.1):\n",
    "        super(AuctionPredictor, self).__init__()\n",
    "        self.encoder = Encoder(input_size, item_index, embedding_size, encoder_hidden_size, dropout_p)\n",
    "        self.decoder = Decoder(encoder_hidden_size, decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(X)\n",
    "        decoder_outputs = self.decoder(encoder_outputs, encoder_hidden)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 16\n",
    "encoder_hidden_size = 32\n",
    "decoder_hidden_size = 32\n",
    "epochs = 1\n",
    "\n",
    "train_dataset = AuctionDataset(conn, pairs, item_to_index)\n",
    "val_dataset = AuctionDataset(conn, val_pairs, item_to_index)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_auctions)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=collate_auctions)\n",
    "\n",
    "model = AuctionPredictor(input_size=5, encoder_hidden_size=encoder_hidden_size, decoder_hidden_size=decoder_hidden_size, item_index=3, embedding_size=embedding_size).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_path='checkpoints'):\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(checkpoint_path, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_file)\n",
    "    print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    eval_steps,\n",
    "    device,\n",
    "    optimizer,\n",
    "    criterion\n",
    "):\n",
    "    print(\"Starting training for\", epochs, \"epochs\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        mse_losses = []\n",
    "        mae_losses = []\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            \n",
    "            loss = criterion(y_pred, y.unsqueeze(2))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mae = F.l1_loss(y_pred, y.unsqueeze(2), reduction='sum')\n",
    "                n = (y != 0).sum().item()\n",
    "                mae /= n\n",
    "                \n",
    "            mae_losses.append(mae.item())\n",
    "            mse_losses.append(loss.item())\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {epoch} Iteration {i} Loss {np.mean(mse_losses)} MAE {np.mean(mae_losses)}\")\n",
    "                mse_losses = []\n",
    "                mae_losses = []\n",
    "                \n",
    "            if i % eval_steps == 0:\n",
    "                evaluate(model, val_loader, device, criterion)\n",
    "        \n",
    "        if epoch  % 5 == 0:\n",
    "            save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device,\n",
    "    criterion\n",
    "):\n",
    "    print(\"Evaluating model\")\n",
    "    model.eval()\n",
    "\n",
    "    mse_losses = []\n",
    "    mae_losses = []\n",
    "\n",
    "    for i, (X, y) in enumerate(val_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = criterion(y_pred, y.unsqueeze(2))\n",
    "        \n",
    "        mae = F.l1_loss(y_pred, y.unsqueeze(2), reduction='sum')\n",
    "        n = (y != 0).sum().item()\n",
    "        mae /= n\n",
    "        \n",
    "        mse_losses.append(loss.item())\n",
    "        mae_losses.append(mae.item())\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            break\n",
    "        \n",
    "    print(f\"Validation loss: {np.mean(mse_losses)} MAE: {np.mean(mae_losses)}\")\n",
    "    model.train()\n",
    "    \n",
    "train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    epochs,\n",
    "    eval_steps=300,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
