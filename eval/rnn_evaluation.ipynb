{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from auction_predictor import AuctionPredictor\n",
    "from auction_dataset import AuctionDataset\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items shape: (10396, 13)\n",
      "Number of unique items: 10396\n",
      "Historical prices loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]/tmp/ipykernel_412/3033789478.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  weekly_historical_prices = pd.concat([weekly_historical_prices, avg_prices])\n",
      "100%|██████████| 66/66 [00:02<00:00, 33.00it/s]\n"
     ]
    }
   ],
   "source": [
    "items = pd.read_csv('../data/items.csv')\n",
    "print(\"Items shape:\", items.shape)\n",
    "n_items = len(items)\n",
    "item_to_index = {item_id: i + 2 for i, item_id in enumerate(items['item_id'])}\n",
    "item_to_index[0] = 0 \n",
    "item_to_index[1] = 1  \n",
    "print(f\"Number of unique items: {n_items}\")\n",
    "\n",
    "historical_prices_path = '../data/historical_prices.csv'\n",
    "if not os.path.exists(historical_prices_path):\n",
    "    historical_prices_path = 'historical_prices.csv'\n",
    "\n",
    "try:\n",
    "    historical_prices = pd.read_csv(historical_prices_path)\n",
    "    historical_prices['datetime'] = pd.to_datetime(historical_prices['datetime'])\n",
    "    print('Historical prices loaded successfully.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The historical prices file {historical_prices_path} was not found.')\n",
    "    historical_prices = pd.DataFrame(columns=['item_id', 'datetime', 'price'])\n",
    "\n",
    "if historical_prices.empty:\n",
    "    print(\"Warning: historical_prices is empty. This may cause issues later.\")\n",
    "\n",
    "average_days = 7\n",
    "dates = sorted(set(pd.Timestamp(dt).strftime('%Y-%m-%d 00:00:00') for dt in historical_prices['datetime'].values))\n",
    "dates = dates[average_days:]\n",
    "\n",
    "weekly_historical_prices = pd.DataFrame(columns=historical_prices.columns)\n",
    "for date in tqdm(dates):\n",
    "    date = pd.to_datetime(date)\n",
    "    date_range = date - pd.Timedelta(days=average_days)\n",
    "\n",
    "    filtered_historical_prices = historical_prices[(historical_prices['datetime'] <= date) & (historical_prices['datetime'] > date_range)]\n",
    "\n",
    "    avg_prices = filtered_historical_prices.groupby('item_id')['price'].mean().reset_index()\n",
    "    avg_prices['datetime'] = date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    weekly_historical_prices = pd.concat([weekly_historical_prices, avg_prices])\n",
    "\n",
    "weekly_historical_prices.head(5)\n",
    "\n",
    "time_left_mapping = {\n",
    "    'VERY_LONG': 48,\n",
    "    'LONG': 12,\n",
    "    'MEDIUM': 2,\n",
    "    'SHORT': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 19500.83it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 44306.03it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 29017.20it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 37131.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading file sample/24-08-2024/20240824T03.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading file sample/24-08-2024/20240824T07.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading file sample/24-08-2024/20240824T10.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading file sample/26-08-2024/20240826T22.json: Expecting value: line 1 column 1 (char 0)\n",
      "Processed auctions for 1530 different items.\n",
      "Example of processed auctions for an item: [23844810.0, 25099790.0, 0.005, 1, 0.010416666666666666, 0.9791666666666666, 25099790.0]\n",
      "Example of hours_on_sale for an auction: (1360771519, 49.0)\n"
     ]
    }
   ],
   "source": [
    "def load_auctions_from_sample(data_dir='sample/'):\n",
    "    file_info = {}\n",
    "    auction_appearances = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for filename in tqdm(files):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            date = datetime.strptime(filename.split('.')[0], '%Y%m%dT%H')\n",
    "            file_info[filepath] = date\n",
    "\n",
    "    file_info = {k: v for k, v in sorted(file_info.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    all_auctions = []\n",
    "    \n",
    "    for filepath in list(file_info.keys()):\n",
    "        with open(filepath, 'r') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                \n",
    "                if 'auctions' not in json_data:\n",
    "                    print(f\"File {filepath} does not contain 'auctions' key, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                auction_data = json_data['auctions']\n",
    "                \n",
    "                if not auction_data:\n",
    "                    print(f\"File {filepath} is empty, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                timestamp = file_info[filepath]\n",
    "                for auction in auction_data:\n",
    "                    auction_id = auction['id']\n",
    "                    auction['timestamp'] = timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    \n",
    "                    if auction_id not in auction_appearances:\n",
    "                        auction_appearances[auction_id] = {'first': timestamp, 'last': timestamp}\n",
    "                    else:\n",
    "                        auction_appearances[auction_id]['last'] = timestamp\n",
    "                \n",
    "                all_auctions.extend(auction_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return all_auctions, auction_appearances\n",
    "\n",
    "def process_auction_data(auctions, auction_appearances, prediction_time):\n",
    "    auctions_by_item = {}\n",
    "    hours_on_sale = {}\n",
    "    \n",
    "    for auction in auctions:\n",
    "        if not isinstance(auction, dict) or 'item' not in auction or 'id' not in auction['item']:\n",
    "            print(f\"Unexpected structure in auction: {auction}\")\n",
    "            continue\n",
    "        auction_id = auction['id']\n",
    "        item_id = auction['item']['id']\n",
    "        time_left_numeric = time_left_mapping.get(auction['time_left'], 0)\n",
    "        bid = auction['bid'] * 10000 / 1000\n",
    "        buyout = auction['buyout'] * 10000 / 1000\n",
    "        quantity = auction['quantity'] / 200\n",
    "        time_left = time_left_numeric / 48\n",
    "        item_index = item_to_index.get(item_id, 1)\n",
    "        timestamp = datetime.strptime(auction['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        if timestamp != prediction_time:\n",
    "            continue\n",
    "\n",
    "        first_appearance = auction_appearances[auction_id]['first']\n",
    "        hours_since_first_appearance = (prediction_time - first_appearance).total_seconds() / 3600\n",
    "        hours_since_first_appearance = hours_since_first_appearance / 48.0\n",
    "        hours_on_sale[auction_id] = (auction_appearances[auction_id]['last'] - first_appearance).total_seconds() / 3600\n",
    "        \n",
    "        datetime_str = prediction_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        if (item_id, datetime_str) in weekly_historical_prices.index:\n",
    "            historical_price = weekly_historical_prices.loc[(item_id, datetime_str), 'price']\n",
    "        else:\n",
    "            historical_price = buyout\n",
    "        \n",
    "        processed_auction = [\n",
    "            bid, \n",
    "            buyout,  \n",
    "            quantity, \n",
    "            item_index,\n",
    "            time_left, \n",
    "            hours_since_first_appearance,  \n",
    "            historical_price  \n",
    "        ]\n",
    "        \n",
    "        if item_index not in auctions_by_item:\n",
    "            auctions_by_item[item_index] = []\n",
    "        \n",
    "        auctions_by_item[item_index].append(processed_auction)\n",
    "    \n",
    "    return auctions_by_item, hours_on_sale\n",
    "\n",
    "prediction_time = datetime.strptime(\"2024-08-25 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "data_dir = 'sample/'\n",
    "auction_data, auction_appearances = load_auctions_from_sample(data_dir)\n",
    "auctions_by_item, hours_on_sale = process_auction_data(auction_data, auction_appearances, prediction_time)\n",
    "\n",
    "print(f\"Processed auctions for {len(auctions_by_item)} different items.\")\n",
    "print(f\"Example of processed auctions for an item: {auctions_by_item[list(auctions_by_item.keys())[0]][0]}\")\n",
    "print(f\"Example of hours_on_sale for an auction: {list(hours_on_sale.items())[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 1174401\n",
      "An error occurred while loading the model: Error(s) in loading state_dict for AuctionPredictor:\n",
      "\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([10398, 64]) from checkpoint, the shape in current model is torch.Size([10396, 64]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412/1915721480.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "encoder_hidden_size = 128\n",
    "decoder_hidden_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model = AuctionPredictor(\n",
    "    n_items=n_items,             \n",
    "    input_size=6,                   \n",
    "    encoder_hidden_size=encoder_hidden_size,\n",
    "    decoder_hidden_size=decoder_hidden_size,\n",
    "    item_index=3,                   \n",
    "    embedding_size=embedding_size,\n",
    "    dropout_p=0.1,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "model_path = 'models/rnn_model.pt'\n",
    "if not os.path.exists(model_path):\n",
    "    model_path = '../eval/models/rnn_model.pt'  \n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  \n",
    "    print('Pre-trained RNN model loaded successfully.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The model file {model_path} was not found.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred while loading the model: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items: 1530\n",
      "Total number of auctions: 7873\n",
      "Hours on sale statistics: Min: 1.0, Max: 50.0, Mean: 38.87031627079893\n",
      "Number of predictions: 7873\n",
      "Number of actual values: 7873\n",
      "Predictions statistics: Min: [-0.12777309], Max: [43.943882], Mean: 9.186800003051758\n",
      "Actual values statistics: Min: 0, Max: 0, Mean: 0.0\n",
      "RNN Model MAE: 9.186833107793602\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rnn_with_inspection_and_mae(model, auctions_by_item, hours_on_sale, prediction_time):\n",
    "    all_predictions = []\n",
    "    all_actual_values = []\n",
    "\n",
    "    print(f\"Total number of items: {len(auctions_by_item)}\")\n",
    "    print(f\"Total number of auctions: {sum(len(auctions) for auctions in auctions_by_item.values())}\")\n",
    "\n",
    "    if hours_on_sale:\n",
    "        print(f\"Hours on sale statistics: Min: {min(hours_on_sale.values())}, Max: {max(hours_on_sale.values())}, Mean: {np.mean(list(hours_on_sale.values()))}\")\n",
    "    else:\n",
    "        print(\"No 'hours_on_sale' data to calculate statistics.\")\n",
    "\n",
    "    for item_idx, auctions in auctions_by_item.items():\n",
    "        if not auctions:\n",
    "            continue\n",
    "        \n",
    "        auctions_np = np.array(auctions)\n",
    "        X = torch.tensor(auctions_np, dtype=torch.float32).to(device)\n",
    "        X = X.unsqueeze(0) \n",
    "        with torch.no_grad():\n",
    "            predictions = model(X)\n",
    "\n",
    "        auction_ids = [auction[0] for auction in auctions]\n",
    "        actual_values = [hours_on_sale.get(auction_id, 0) for auction_id in auction_ids]\n",
    "\n",
    "        if len(predictions.squeeze(0)) == len(actual_values):  \n",
    "            all_predictions.extend(predictions.squeeze(0).cpu().numpy())\n",
    "            all_actual_values.extend(actual_values)\n",
    "        else:\n",
    "            print(f\"Skipping item {item_idx} due to size mismatch: {len(predictions.squeeze(0))} predictions vs {len(actual_values)} actual values.\")\n",
    "\n",
    "    if not all_predictions:\n",
    "        print(\"No valid auctions were processed. Check your data.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Number of predictions: {len(all_predictions)}\")\n",
    "    print(f\"Number of actual values: {len(all_actual_values)}\")\n",
    "    print(f\"Predictions statistics: Min: {min(all_predictions)}, Max: {max(all_predictions)}, Mean: {np.mean(all_predictions)}\")\n",
    "    print(f\"Actual values statistics: Min: {min(all_actual_values)}, Max: {max(all_actual_values)}, Mean: {np.mean(all_actual_values)}\")\n",
    "\n",
    "    return all_predictions, all_actual_values\n",
    "\n",
    "def calculate_mae(all_predictions, all_actual_values):\n",
    "    if len(all_predictions) == 0 or len(all_actual_values) == 0:\n",
    "        print(\"No valid data for MAE calculation.\")\n",
    "        return None\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actual_values = np.array(all_actual_values)\n",
    "    mae = mean_absolute_error(all_actual_values, all_predictions)\n",
    "    return mae\n",
    "all_predictions, all_actual_values = evaluate_rnn_with_inspection_and_mae(\n",
    "    model, auctions_by_item, hours_on_sale, prediction_time\n",
    ")\n",
    "if all_predictions is not None and all_actual_values is not None:\n",
    "    rnn_mae = calculate_mae(all_predictions, all_actual_values)\n",
    "    if rnn_mae is not None:\n",
    "        print(f'RNN Model MAE: {rnn_mae}')\n",
    "    else:\n",
    "        print('Evaluation failed due to lack of valid data.')\n",
    "else:\n",
    "    print('No predictions were made.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
