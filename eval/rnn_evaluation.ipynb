{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from auction_predictor import AuctionPredictor\n",
    "from auction_dataset import AuctionDataset\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('../data/items.csv')\n",
    "print(\"Items shape:\", items.shape)\n",
    "n_items = len(items)\n",
    "item_to_index = {item_id: i + 2 for i, item_id in enumerate(items['item_id'])}\n",
    "item_to_index[0] = 0 \n",
    "item_to_index[1] = 1  \n",
    "print(f\"Number of unique items: {n_items}\")\n",
    "\n",
    "historical_prices_path = '../data/historical_prices.csv'\n",
    "if not os.path.exists(historical_prices_path):\n",
    "    historical_prices_path = 'historical_prices.csv'\n",
    "\n",
    "try:\n",
    "    weekly_historical_prices = pd.read_csv(historical_prices_path)\n",
    "    weekly_historical_prices['datetime'] = weekly_historical_prices['datetime'].astype(str)\n",
    "    weekly_historical_prices.set_index(['item_id', 'datetime'], inplace=True)\n",
    "    print('Historical prices loaded successfully.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The historical prices file {historical_prices_path} was not found.')\n",
    "    weekly_historical_prices = pd.DataFrame(columns=['item_id', 'datetime', 'price'])\n",
    "\n",
    "time_left_mapping = {\n",
    "    'VERY_LONG': 48,\n",
    "    'LONG': 12,\n",
    "    'MEDIUM': 2,\n",
    "    'SHORT': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_auction_data(auctions, max_auctions_per_item=1000):\n",
    "    auctions_by_item = {}\n",
    "    \n",
    "    for auction in auctions:\n",
    "        if not isinstance(auction, dict) or 'item' not in auction or 'id' not in auction['item']:\n",
    "            print(f\"Unexpected structure in auction: {auction}\")\n",
    "            continue\n",
    "\n",
    "        item_id = auction['item']['id']\n",
    "        time_left_numeric = time_left_mapping.get(auction['time_left'], 0)\n",
    "        bid = auction['bid'] / 100  \n",
    "        quantity = auction['quantity'] / 200\n",
    "        time_left = time_left_numeric / 48\n",
    "        item_index = item_to_index.get(item_id, len(item_to_index)) \n",
    "        \n",
    "        processed_auction = [\n",
    "            bid, \n",
    "            auction['buyout'] / 100,  \n",
    "            quantity, \n",
    "            item_index,  \n",
    "            time_left, \n",
    "            0, \n",
    "            0   \n",
    "        ]\n",
    "        \n",
    "        if item_index not in auctions_by_item:\n",
    "            auctions_by_item[item_index] = []\n",
    "        \n",
    "        if len(auctions_by_item[item_index]) < max_auctions_per_item:\n",
    "            auctions_by_item[item_index].append(processed_auction)\n",
    "    \n",
    "    return auctions_by_item\n",
    "\n",
    "def load_auctions_from_sample(data_dir='sample/'):\n",
    "    file_info = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for filename in tqdm(files):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            date = datetime.strptime(filename.split('.')[0], '%Y%m%dT%H')\n",
    "            file_info[filepath] = date\n",
    "\n",
    "    file_info = {k: v for k, v in sorted(file_info.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    all_auctions = []\n",
    "    \n",
    "    for filepath in list(file_info.keys()):\n",
    "        with open(filepath, 'r') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                \n",
    "                if 'auctions' not in json_data:\n",
    "                    print(f\"File {filepath} does not contain 'auctions' key, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                auction_data = json_data['auctions']\n",
    "                \n",
    "                if not auction_data:\n",
    "                    print(f\"File {filepath} is empty, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                all_auctions.extend(auction_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading file {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return all_auctions\n",
    "\n",
    "data_dir = 'sample/'\n",
    "auction_data = load_auctions_from_sample(data_dir)\n",
    "auctions_by_item = process_auction_data(auction_data)\n",
    "\n",
    "print(f\"Processed auctions for {len(auctions_by_item)} different items.\")\n",
    "print(f\"Example of processed auctions for an item: {auctions_by_item[list(auctions_by_item.keys())[0]][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "encoder_hidden_size = 128\n",
    "decoder_hidden_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model = AuctionPredictor(\n",
    "    n_items=n_items,             \n",
    "    input_size=7,                   \n",
    "    encoder_hidden_size=encoder_hidden_size,\n",
    "    decoder_hidden_size=decoder_hidden_size,\n",
    "    item_index=3,                   \n",
    "    embedding_size=embedding_size,\n",
    "    dropout_p=0.1,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "model_path = 'models/rnn_model.pt'\n",
    "if not os.path.exists(model_path):\n",
    "    model_path = '../eval/models/rnn_model.pt'  \n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  \n",
    "    print('Pre-trained RNN model loaded successfully.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The model file {model_path} was not found.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred while loading the model: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, auctions_by_item):\n",
    "    all_predictions = []\n",
    "    all_actual_values = []\n",
    "\n",
    "    for item_index, auctions in auctions_by_item.items():\n",
    "        if not auctions: \n",
    "            continue\n",
    "        auctions_np = np.array(auctions)\n",
    "        data = auctions_np[:, :-1].tolist()  \n",
    "        y = auctions_np[:, -1]\n",
    "        data = [data]\n",
    "        data_np = np.array(data)\n",
    "        X = torch.tensor(data_np, dtype=torch.float32).to(device)\n",
    "        print(f\"Item {item_index}:\")\n",
    "        print(f\"Input shape (X): {X.shape}\")\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X)\n",
    "        \n",
    "        print(f\"Predictions shape: {predictions.shape}\")\n",
    "        \n",
    "        all_predictions.extend(predictions.squeeze(0).cpu().numpy())\n",
    "        all_actual_values.extend(y)\n",
    "\n",
    "        print(\"First 5 predictions:\")\n",
    "        print(\"Predicted:\\t\", predictions.squeeze(0)[:5].cpu().numpy())\n",
    "        print(\"Actual:\\t\\t\", y[:5])\n",
    "        print()\n",
    "\n",
    "    if not all_predictions: \n",
    "        print(\"No valid auctions were processed. Check your data.\")\n",
    "        return None\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_actual_values = np.array(all_actual_values)\n",
    "\n",
    "    mae = mean_absolute_error(all_actual_values, all_predictions)\n",
    "    return mae\n",
    "\n",
    "rnn_mae = evaluate_rnn(model, auctions_by_item)\n",
    "if rnn_mae is not None:\n",
    "    print(f'RNN Model MAE: {rnn_mae}')\n",
    "else:\n",
    "    print('Evaluation failed due to lack of valid data.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
