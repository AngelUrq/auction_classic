# Auction Transformer Training Configuration
resume: null

data:
  dir: generated
  date_start: "2025-07-01"
  date_end: "2025-09-01"
  train_split: 0.95
  train_fraction: 0.90
  max_hours_back: 24
  max_sequence_length: 1024
  bucket_sampling: true  # Group similar-length sequences to reduce padding waste
  num_buckets: 20        # Number of length buckets (only used if bucket_sampling=true)
  prefetch_factor: 8

training:
  num_epochs: 1
  batch_size: 64
  learning_rate: 1e-4
  gradient_clip: 3.0
  precision: bf16-mixed
  val_check_interval: 0.1
  limit_val_batches: 3000
  checkpoint_every: 10000
  log_every: 10
  logging_interval: 1000
  num_workers: 8

model:
  input_size: 5
  embedding_dim: 32
  d_model: 64
  dim_feedforward: 256
  nhead: 4
  num_layers: 4
  dropout: 0.0
  quantiles: [0.1, 0.5, 0.9]

hardware:
  accelerator: auto
  devices: 1

logging:
  wandb: true
  project: auction_transformer

checkpoint:
  dir: models/
  save_last: true
  save_top_k: -1
