# Auction Transformer Training Configuration
data:
  dir: generated
  date_start: "2025-12-25"
  date_end: "2026-02-15"
  train_split: 0.95
  train_fraction: 0.90
  max_hours_back: 24
  max_sequence_length: 1024
  bucket_sampling: true  # Group similar-length sequences to reduce padding waste
  num_buckets: 20        # Number of length buckets (only used if bucket_sampling=true)
  prefetch_factor: 8

training:
  num_epochs: 1
  batch_size: 64
  learning_rate: 1e-5
  use_lr_scheduler: true
  gradient_clip: 3.0
  precision: bf16
  val_check_interval: 0.1
  limit_val_batches: 5000
  checkpoint_every: 10000
  log_every: 10
  logging_interval: 1000
  num_workers: 8

model:
  pretrained_path: null
  reset_embeddings: false
  input_size: 5
  embedding_dim: 32
  d_model: 256
  dim_feedforward: 1024
  nhead: 16
  num_layers: 4
  dropout: 0.1
  n_buyout_ranks: 64
  n_time_bins: 48
  quantiles: [0.1, 0.5, 0.9]
  pinball_loss_weight: 1.0
  classification_loss_weight: 70
  classification_pos_weight: 3.427
  deephit_nll_weight: 1.0       # pycox alpha: loss = nll_weight * NLL + (1 - nll_weight) * rank_loss
  deephit_ranking_sigma: 0.1   # kernel bandwidth for ranking loss (pycox sigma)

hardware:
  accelerator: auto
  devices: 1

logging:
  wandb: true
  project: auction_transformer

checkpoint:
  dir: models/
  save_last: true
  save_top_k: -1
