# Auction Transformer Training Configuration
data:
  dir: generated
  date_start: "2025-12-20"
  date_end: "2026-01-10"
  train_split: 0.95
  train_fraction: 0.90
  max_hours_back: 72
  max_sequence_length: 4096
  bucket_sampling: true  # Group similar-length sequences to reduce padding waste
  num_buckets: 20        # Number of length buckets (only used if bucket_sampling=true)
  prefetch_factor: 8

training:
  num_epochs: 1
  batch_size: 64
  learning_rate: 1e-3
  use_lr_scheduler: true
  gradient_clip: 3.0
  precision: bf16
  val_check_interval: 0.1
  limit_val_batches: 5000
  checkpoint_every: 10000
  log_every: 10
  logging_interval: 1000
  num_workers: 8

model:
  pretrained_path: /home/angel/source/auction_classic/models/transformer-4.2M-quantile-historical_72-lr1e-04-bs64/epoch_epoch=00-v3.ckpt
  reset_embeddings: true
  input_size: 5
  embedding_dim: 32
  d_model: 64
  dim_feedforward: 256
  nhead: 4
  num_layers: 4
  dropout: 0.0
  quantiles: [0.1, 0.5, 0.9]
  classification_threshold_hours: 8.0
  classification_loss_weight: 1.0

hardware:
  accelerator: auto
  devices: 1

logging:
  wandb: true
  project: auction_transformer

checkpoint:
  dir: models/
  save_last: true
  save_top_k: -1
